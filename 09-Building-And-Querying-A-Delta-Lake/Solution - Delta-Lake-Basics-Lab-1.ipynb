{"cells":[{"cell_type":"markdown","source":["# Delta Lake Basics Lab Solution\n\nDatabricks&reg; Delta allows you to read, write and query data in data lakes in an efficient manner.\n\n## Learning Objectives:\nIn this lab, you will:\n* Create a new Delta Lake from aggregate data of an existing Delta Lake\n* UPSERT records into a Delta lake\n* Append new data to an existing Delta Lake\n\n## Audience\n* Primary Audience: Data Engineers\n* Secondary Audience: Data Analysts and Data Scientists\n\n## Prerequisites\n* Web browser: current versions of Google Chrome, Firefox, Safari, Microsoft Edge and\nInternet Explorer 11 on Windows 7, 8, or 10 (see <a href=\"https://docs.databricks.com/user-guide/supported-browsers.html#supported-browsers#\" target=\"_blank\">Supported Web Browsers</a>)\n* Databricks Runtime 4.2 or greater\n\n## Datasets Used\nWe will use online retail datasets from `/mnt/training/online_retail`"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> **This lab depends upon the complete execution of the notebook titled \"Open-Source-Delta-Lake\" which registered the `customer_data_delta` table. If this table doesn't exist, run the cell below.**"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Delta-Lab-1-Prep\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Because we'll be calculating some aggregates in this notebook, we'll change our partitions after shuffle from the default `200` to `8` (which is a good number for the 8 node cluster we're currently working on)."],"metadata":{}},{"cell_type":"code","source":["%python\n\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["-sandbox\n## What is a table? \nBefore we continue, we need to address a semantic concern addressed by the [Databricks docs](https://docs.databricks.com/user-guide/tables.html#view-databases-and-tables):\n\n> A Databricks table is a collection of structured data. Tables are equivalent to Apache Spark DataFrames.\n\nGenerally, the distinction between tables and DataFrames in Spark can be summarized by discussing scope and persistence:\n- Tables are defined at the **workspace** level and **persist** between notebooks.\n- DataFrames are defined at the **notebook** level and are **ephemeral**.\n\nWhen we discuss **Delta tables**, we are always talking about collections of structured data that persist between notebooks. Importantly, we do not need to register a directory of files to Spark SQL in order to refer to them as a table. The directory of files itself _is_ the table; registering it with a useful name to Spark SQL just gives us easy accessing to querying these underlying data.\n\nA **Delta Lake** can be thought of as a collection of one or many Delta tables. Generally, an entire elastic storage container will be dedicated to a single Delta Lake, and data will be enriched and cleaned as it is promoted through pre-defined logic.\n\n<img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> To make Delta tables easily accessible, register them using Spark SQL. Use table ACLs to control access in workspaces shared by many diverse parties within an organization."],"metadata":{}},{"cell_type":"markdown","source":["### Creating a new Delta table\n\nYou business intelligence team wants to create a dashboard to track the total number of orders made by customers globally. Many of your customers are international retailers, and have the same customer ID.\n\nBecause you batch process your data each day, you've decided to create a workflow that will update their numbers when you run your reports each night.\n\nIn this notebook, we'll start by transforming existing data stored in Delta to create a new Delta table for your BI team's dashboard. Then, we'll create processes to append new data to our full records as well as updating the Delta table for the BI team."],"metadata":{}},{"cell_type":"markdown","source":["### Load in Delta table\n\nThe path to our existing Delta table is provided."],"metadata":{}},{"cell_type":"code","source":["DeltaPath = userhome + \"/delta/customer-data/\""],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Note that because we registered a global table associated with this Delta table, you should already be able to query this data with SQL. To see all your currently registered tables, click the `Data` icon on the left navigation bar.\n\n<img src=https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/adbcore/data-button.png width=100px>"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT COUNT(*) FROM customer_data_delta"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Because we stored our data in Delta, our schema and partions are preserved. All we'll need to do is specify the format and the path."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\ndeltaDF = (spark.read\n  .format(\"delta\")\n  .load(DeltaPath))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Generate our BI table\nWe'll start out by just looking at our aggregate counts. Here, we group by both `\"CustomerID\"` and `\"Country\"`, as it is the combination of these two fields that is of interest to our BI team."],"metadata":{}},{"cell_type":"code","source":["customerCounts = (deltaDF.groupBy(\"CustomerID\", \"Country\")\n  .count()\n  .withColumnRenamed(\"count\", \"total_orders\"))\n\ndisplay(customerCounts)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Clicking on the names of the various columns will allow us to quickly sort on different fields. You may notice that we have a large number of entries that are `null` for both `\"CustomerID\"` and `\"Country\"`. While in production, we would like to explore _why_ we are seeing these missing values, for now we'll just leave them as is and save out this DataFrame as a new Delta table.\n\nHere, the path is provided for you."],"metadata":{}},{"cell_type":"code","source":["CustomerCountsPath = userhome + \"/delta/customer_counts/\"\n\ndbutils.fs.rm(CustomerCountsPath, True) #deletes Delta table if previously created"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Here we'll write out our Delta table to the path provided above. Make sure the following settings are provided:\n- `overwrite` (so that this code will work if you run it again)\n- format as `delta`\n- partition by `\"Country\"`\n- save to `CustomerCountsPath`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\n(customerCounts.write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"Country\")\n  .save(CustomerCountsPath))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["We'll also register this Delta table as a Spark SQL table."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  DROP TABLE IF EXISTS customer_counts\n\"\"\")\n\nspark.sql(\"\"\"\n  CREATE TABLE customer_counts\n  USING DELTA\n  LOCATION '{}'\n\"\"\".format(CustomerCountsPath))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Now our BI team can quickly query those data points they've expressed interest in."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT *\nFROM customer_counts"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Looking at your existing Delta table, you know that a large number of recent orders haven't been loaded in yet."],"metadata":{}},{"cell_type":"markdown","source":["###  READ updated CSV data\n\nRead the data into a DataFrame. We'll use the same schema that we used when creating our Delta table, which is supplied for you."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n\ninputSchema = StructType([\n  StructField(\"InvoiceNo\", IntegerType(), True),\n  StructField(\"StockCode\", StringType(), True),\n  StructField(\"Description\", StringType(), True),\n  StructField(\"Quantity\", IntegerType(), True),\n  StructField(\"InvoiceDate\", StringType(), True),\n  StructField(\"UnitPrice\", DoubleType(), True),\n  StructField(\"CustomerID\", IntegerType(), True),\n  StructField(\"Country\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Read data in `newDataPath`. Re-use `inputSchema` as defined above. We'll name our DataFrame `newDataDF`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nnewDataPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-small.csv\"\nnewDataDF = (spark\n  .read\n  .option(\"header\", \"true\")\n  .schema(inputSchema)\n  .csv(newDataPath)\n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Let's do the same aggregate count as above, on `\"Country\"` and `\"CustomerID\"`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nnewCustomerCounts = (newDataDF.groupBy(\"CustomerID\", \"Country\")\n  .count()\n  .withColumnRenamed(\"count\", \"total_orders\"))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(newCustomerCounts)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### UPSERT new customer counts\n\nNow that we've successfully loaded and aggregated our new data, we can upsert it in our existing Delta Lake.\n\nFirst, we'll register it as a temp view."],"metadata":{}},{"cell_type":"code","source":["newCustomerCounts.createOrReplaceTempView(\"new_customer_counts\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["And now we can merge these new counts into our existing data."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nMERGE INTO customer_counts\nUSING new_customer_counts\nON customer_counts.Country = new_customer_counts.Country\nAND customer_counts.CustomerID = new_customer_counts.CustomerID\nWHEN MATCHED THEN\n  UPDATE SET total_orders = customer_counts.total_orders + new_customer_counts.total_orders\nWHEN NOT MATCHED THEN\n  INSERT *"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["We can write a simple SQL query to confirm that this has worked."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT SUM(total_orders) FROM customer_counts"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### Update full records using append\n\nWe want to retain the full records being generated from our batch report in our existing non-aggregated Delta table.\n\nIn this case, we're assuming that the records we process at the end of each day are correct, and that batch processing will result in correct, stable records. We can safely write our table to the same file path using the append mode to insert these records.\n\n**Note**: If our reports included changes to line items from previous days, we would want to write an UPSERT which would allow us to simultaneously update our changed records and insert new data."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\n(newDataDF.write\n  .format(\"delta\")\n  .mode(\"append\")\n  .save(DeltaPath))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Querying our table again shows that we've immediately updated."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM customer_data_delta"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-batch.html#\" target=\"_blank\">Table Batch Read and Writes</a>"],"metadata":{}}],"metadata":{"name":"Solution - Delta-Lake-Basics-Lab-1","notebookId":1947196994888434},"nbformat":4,"nbformat_minor":0}
