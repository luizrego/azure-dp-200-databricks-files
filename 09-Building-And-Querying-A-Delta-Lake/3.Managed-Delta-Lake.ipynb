{"cells":[{"cell_type":"markdown","source":["# <img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=80px> Managed Delta Lake\n\nDelta Lake&reg; managed and queried via the Databricks platform includes additional features and optimizations.\n\nThese include:\n\n- **Optimize**\n\n- **Data skipping**\n\n- **Z-Order**\n\n- **Caching**\n\n<img src=\"https://www.evernote.com/l/AAGv1SuWeRNJM4TI4bIOyGNPm0CTHa17PLwB/image.png\" width=900px>"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run ./Includes/Delta-Optimization-Setup"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["spark.sql(\"\"\"\n    DROP TABLE IF EXISTS iot_data\n  \"\"\")\nspark.sql(\"\"\"\n    CREATE TABLE iot_data\n    USING DELTA\n    LOCATION '{}/delta/iot-events/'\n  \"\"\".format(userhome))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["iotPath = userhome + \"/delta/iot-events/\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## SMALL FILE PROBLEM\n\nHistorical and new data is often written in very small files and directories.\n\nThis data may be spread across a data center or even across the world (that is, not co-located).\n\nThe result is that a query on this data may be very slow due to\n* network latency\n* volume of file metatadata\n\nThe solution is to compact many small files into one larger file.\nDelta Lake has a mechanism for compacting small files."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### OPTIMIZE\nDelta Lake supports the `OPTIMIZE` operation, which performs file compaction.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Small files are compacted together into new larger files up to 1GB.\nThus, at this point the number of files increases!\n\nThe 1GB size was determined by the Databricks optimization team as a trade-off between query speed and run-time performance when running Optimize.\n\n`OPTIMIZE` is not run automatically because you must collect many small files first.\n\n* Run `OPTIMIZE` more often if you want better end-user query performance\n* Since `OPTIMIZE` is a time consuming step, run it less often if you want to optimize cost of compute hours\n* To start with, run `OPTIMIZE` on a daily basis (preferably at night when spot prices are low), and determine the right frequency for your particular business case\n* In the end, the frequency at which you run `OPTIMIZE` is a business decision\n\nThe easiest way to see what `OPTIMIZE` does is to perform a simple `count(*)` query before and after and compare the timing!"],"metadata":{}},{"cell_type":"markdown","source":["Take a look at the `iotPath + \"/date=2018-06-01/\" ` directory.\n\nNotice, in particular files like `../delta/iot-events/date=2018-07-26/part-xxxx.snappy.parquet`. There are hundreds of small files!"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(iotPath + \"/date=2016-07-26\"))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["CAUTION: Run this query. Notice it is very slow, due to the number of small files."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM iot_data where deviceId=92"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n### Data Skipping and ZORDER\n\nDelta Lake uses two mechanisms to speed up queries.\n\n<b>Data Skipping</b> is a performance optimization that aims at speeding up queries that contain filters (WHERE clauses).\n\nFor example, we have a data set that is partitioned by `date`.\n\nA query using `WHERE date > 2016-07-26` would not access data that resides in partitions that correspond to dates prior to `2016-07-26`.\n\n<b>ZOrdering</b> is a technique to colocate related information in the same set of files.\n\nZOrdering maps multidimensional data to one dimension while preserving locality of the data points.\n\nGiven a column that you want to perform ZORDER on, say `OrderColumn`, Delta\n* takes existing parquet files within a partition\n* maps the rows within the parquet files according to `OrderColumn` using the algorithm described <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">here</a>\n* (in the case of only one column, the mapping above becomes a linear sort)\n* rewrites the sorted data into new parquet files\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You cannot use the partition column also as a ZORDER column."],"metadata":{}},{"cell_type":"markdown","source":["#### ZORDER Technical Overview\n\nA brief example of how this algorithm works (refer to [this blog](https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html) for more details):\n\n![](https://files.training.databricks.com/images/adbcore/zorder.png)\n\nLegend:\n- Gray dot = data point e.g., chessboard square coordinates\n- Gray box = data file; in this example, we aim for files of 4 points each\n- Yellow box = data file that’s read for the given query\n- Green dot = data point that passes the query’s filter and answers the query\n- Red dot = data point that’s read, but doesn’t satisfy the filter; “false positive”"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n\n#### ZORDER example\nIn the image below, table `Students` has 4 columns:\n* `gender` with 2 distinct values\n* `Pass-Fail` with 2 distinct values\n* `Class` with 4 distinct values\n* `Student` with many distinct values\n\nSuppose you wish to perform the following query:\n\n```SELECT Name FROM Students WHERE gender = 'M' AND Pass_Fail = 'P' AND Class = 'Junior'```\n\n```ORDER BY Gender, Pass_Fail```\n\nThe most effective way of performing that search is to order the data starting with the largest set, which is `Gender` in this case.\n\nIf you're searching for `gender = 'M'`, then you don't even have to look at students with `gender = 'F'`.\n\nNote that this technique only works if all `gender = 'M'` values are co-located.\n\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/zorder.png\" style=\"height: 300px\"/></div><br/>"],"metadata":{}},{"cell_type":"markdown","source":["#### ZORDER usage\n\nWith Delta Lake the notation is:\n\n> `OPTIMIZE Students`<br>\n`ZORDER BY Gender, Pass_Fail`\n\nThis will ensure all the data backing `Gender = 'M' ` is colocated, then data associated with `Pass_Fail = 'P' ` is colocated.\n\nSee References below for more details on the algorithms behind ZORDER.\n\nUsing ZORDER, you can order by multiple columns as a comma separated list; however, the effectiveness of locality drops.\n\nIn streaming, where incoming events are inherently ordered (more or less) by event time, use `ZORDER` to sort by a different column, say 'userID'."],"metadata":{}},{"cell_type":"code","source":["%sql\nOPTIMIZE iot_data\nZORDER by (deviceId)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql\nSELECT * FROM iot_data WHERE deviceId=92"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["-sandbox\n## VACUUM\n\nTo save on storage costs you should occasionally clean up invalid files using the `VACUUM` command.\n\nInvalid files are small files compacted into a larger file with the `OPTIMIZE` command.\n\nThe  syntax of the `VACUUM` command is\n>`VACUUM name-of-table RETAIN number-of HOURS;`\n\nThe `number-of` parameter is the <b>retention interval</b>, specified in hours.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Databricks does not recommend you set a retention interval shorter than seven days because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table.\n\nThe scenario here is:\n0. User A starts a query off uncompacted files, then\n0. User B invokes a `VACUUM` command, which deletes the uncompacted files\n0. User A's query fails because the underlying files have disappeared\n\nInvalid files can also result from updates/upserts/deletions.\n\nMore details are provided here: <a href=\"https://docs.databricks.com/delta/optimizations.html#garbage-collection\" target=\"_blank\"> Garbage Collection</a>."],"metadata":{}},{"cell_type":"code","source":["len(dbutils.fs.ls(iotPath + \"date=2016-07-26\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> In the example below we set off an immediate `VACUUM` operation with an override of the retention check so that all files are cleaned up immediately.\n\nDo not do this in production!\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If using Databricks Runtime 5.1, in order to use a retention time of 0 hours, the following flag must be set."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%sql\n\nVACUUM iot_data RETAIN 0 HOURS;"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how the directory looks vastly cleaned up!"],"metadata":{}},{"cell_type":"code","source":["len(dbutils.fs.ls(iotPath + \"date=2016-07-26\"))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Summary\nDelta Lake offers key features that allow for query optimization and garbage collection, resulting in improved performance."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/optimizations.html#\" target=\"_blank\">Optimizing Performance and Cost</a>\n* <a href=\"http://parquet.apache.org/documentation/latest/\" target=\"_blank\">Parquet Metadata</a>\n* <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">Z-Order Curve</a>"],"metadata":{}}],"metadata":{"name":"3.Managed-Delta-Lake","notebookId":1947196994888255},"nbformat":4,"nbformat_minor":0}
