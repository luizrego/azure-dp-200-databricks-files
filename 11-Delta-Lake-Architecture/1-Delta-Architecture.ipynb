{"cells":[{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=\"80px\"/>\n\n# Unifying Structured Streaming with Batch Jobs with Delta Lake\n\nIn this notebook, we will explore combining streaming and batch processing with a single pipeline. We will begin by defining the following logic:\n\n- ingest streaming JSON data from disk and write it to a Delta Lake Table `/activity/Bronze`\n- perform a Stream-Static Join on the streamed data to add additional geographic data\n- transform and load the data, saving it out to our Delta Lake Table `/activity/Silver`\n- summarize the data through aggregation into the Delta Lake Table `/activity/Gold/groupedCounts`\n- materialize views of our gold table through streaming plots and static queries\n\nWe will then demonstrate that by writing batches of data back to our bronze table, we can trigger the same logic on newly loaded data and propagate our changes automatically."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["-sandbox\n## Set up relevant Delta Lake paths\n\nThese paths will serve as the file locations for our Delta Lake tables.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Each streaming write has its own checkpoint directory.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> You cannot write out new Delta files within a repository that contains Delta files. Note that our hierarchy here isolates each Delta table into its own directory."],"metadata":{}},{"cell_type":"code","source":["activityPath = userhome + \"/activity\"\n\nactivityBronzePath = activityPath + \"/Bronze\"\nactivityBronzeCheckpoint = activityBronzePath + \"/checkpoint\"\n\nactivitySilverPath = activityPath + \"/Silver\"\nactivitySilverCheckpoint = activitySilverPath + \"/checkpoint\"\n\nactivityGoldPath = activityPath + \"/Gold\"\ngroupedCountPath = activityGoldPath + \"/groupedCount\"\ngroupedCountCheckpoint = groupedCountPath + \"/checkpoint\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Reset Pipeline\n\nTo reset the pipeline, run the following:"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(activityPath, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: False</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["-sandbox\n## Datasets Used\nThis notebook will consume cell phone accelerometer data. Records have been downsampled so that the streaming data represents less than 3% of the total data being produced. The remainder will be processed as batches.\n\nThe following fields are present:\n\n- `Index`\n- `Arrival_Time`\n- `Creation_Time`\n- `x`\n- `y`\n- `z`\n- `User`\n- `Model`\n- `Device`\n- `gt`\n- `geolocation`\n\n## Define Schema\n\nFor streaming jobs, we need to define our schema before we start.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll reuse this same schema later in the notebook to define our batch processing, which will eliminate the jobs triggered by eliminating a file scan AND enforce the schema that we've defined here."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, LongType, StringType, DoubleType\n\nschema = StructType([\n  StructField(\"Arrival_Time\",LongType()),\n  StructField(\"Creation_Time\",LongType()),\n  StructField(\"Device\",StringType()),\n  StructField(\"Index\",LongType()),\n  StructField(\"Model\",StringType()),\n  StructField(\"User\",StringType()),\n  StructField(\"geolocation\",StructType([\n    StructField(\"city\",StringType()),\n    StructField(\"country\",StringType())\n  ])),\n  StructField(\"gt\",StringType()),\n  StructField(\"id\",LongType()),\n  StructField(\"x\",DoubleType()),\n  StructField(\"y\",DoubleType()),\n  StructField(\"z\",DoubleType())\n])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["-sandbox\n\n### Define Streaming Load from Files in Blob\n\nOur streaming source directory has 36 JSON files of 5k records each saved in a repository. Here, we'll trigger processing on files one at a time. \n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In a production setting, this same logic would allow us to only read new files written to our source directory. We could define `maxFilesPerTrigger` to control the amount of data we consume with each load, or omit this option to consume all new data on disk since the last time the stream has processed."],"metadata":{}},{"cell_type":"code","source":["rawEventsDF = (spark\n  .readStream\n  .format(\"json\")\n  .schema(schema)\n  .option(\"maxFilesPerTrigger\", 1)\n  .load(\"/mnt/training/definitive-guide/data/activity-json/streaming\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["-sandbox\n### WRITE Stream using Delta Lake\n\n#### General Notation\nUse this format to write a streaming job to a Delta Lake table.\n\n<pre>\n(myDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .outputMode(\"append\")\n  .start(path)\n)\n</pre>\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> While we _can_ write directly to tables using the `.table()` notation, this will create fully managed tables by writing output to a default location on DBFS. This is not best practice for production jobs.\n\n#### Output Modes\nNotice, besides the \"obvious\" parameters, specify `outputMode`, which can take on these values\n* `append`: add only new records to output sink\n* `complete`: rewrite full output - applicable to aggregations operations\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> At present, `update` mode is **not** supported for streaming Delta jobs.\n\n#### Checkpointing\n\nWhen defining a Delta Lake streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n\n`.writeStream.format(\"delta\").option(\"checkpointLocation\", <path-to-checkpoint-directory>) ...`\n\nThis is actually a structured streaming feature. It stores the current state of your streaming job.\n\nShould your streaming job stop for some reason and you restart it, it will continue from where it left off.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Also note that every streaming job should have its own checkpoint directory: no sharing."],"metadata":{}},{"cell_type":"code","source":["(rawEventsDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", activityBronzeCheckpoint)\n  .outputMode(\"append\")\n  .start(activityBronzePath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f6df804c450&gt;</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Load Static Lookup Table\n\nBefore enriching our bronze data, we will load a static lookup table for our country codes.\n\nHere, we'll use a parquet file that contains countries and their associated codes and abbreviations.\n\nWhile we can load this as a table (which will copy all files to the workspace and make it available to all users), here we'll manipulate it as a DataFrame."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ngeoForLookupDF = (spark\n  .read\n  .format(\"parquet\")\n  .load(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet/\")\n  .select(col(\"EnglishShortName\").alias(\"country\"), col(\"alpha3Code\").alias(\"countryCode3\")))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["-sandbox\n\n## Create QUERY tables (aka \"silver tables\")\n\nOur current bronze table contains nested fields, as well as time data that has been encoded in non-standard unix time (`Arrival_Time` is encoded as milliseconds from epoch, while `Creation_Time` records nanoseconds between record creation and receipt). \n\nWe also wish to enrich our data with 3 letter country codes for mapping purposes, which we'll obtain from a join with our `geoForLookupDF`.\n\nIn order to parse the data in human-readable form, we create query/silver tables out of the raw data.\n\nWe will stream from our previous file write, define transformations, and rewrite our data to disk.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we do not need to specify a schema when loading Delta files: it is inferred from the metadata!\n\nThe fields of a complex object can be referenced with a \"dot\" notation as in:\n\n`col(\"geolocation.country\")`\n\n\nA large number of these fields/columns can become unwieldy.\n\nFor that reason, it is common to extract the sub-fields and represent them as first-level columns as seen below:"],"metadata":{}},{"cell_type":"markdown","source":["NOTE: You will not be able to run this command until the `rawEventsDF` has initialized."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import from_unixtime\n\nparsedEventsDF = (spark.readStream\n  .format(\"delta\")\n  .load(activityBronzePath)\n  .select(from_unixtime(col(\"Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"Device\"),\n          col(\"Index\"),\n          col(\"Model\"),\n          col(\"User\"),\n          col(\"gt\"),\n          col(\"x\"),\n          col(\"y\"),\n          col(\"z\"),\n          col(\"geolocation.country\").alias(\"country\"),\n          col(\"geolocation.city\").alias(\"city\"))\n  .join(geoForLookupDF, [\"country\"], \"left\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Write to QUERY Tables (aka \"silver tables\")"],"metadata":{}},{"cell_type":"code","source":["(parsedEventsDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", activitySilverCheckpoint)\n  .outputMode(\"append\")\n  .start(activitySilverPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f6df839e310&gt;</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["See contents of Silver directory."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.ls(activitySilverPath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [FileInfo(path=&#39;dbfs:/user/lregoazd@outlook.com/activity/Silver/_delta_log/&#39;, name=&#39;_delta_log/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/user/lregoazd@outlook.com/activity/Silver/checkpoint/&#39;, name=&#39;checkpoint/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/user/lregoazd@outlook.com/activity/Silver/part-00000-a55fb8d0-c22e-4637-8c96-d4198eb555e3-c000.snappy.parquet&#39;, name=&#39;part-00000-a55fb8d0-c22e-4637-8c96-d4198eb555e3-c000.snappy.parquet&#39;, size=1288095),\n FileInfo(path=&#39;dbfs:/user/lregoazd@outlook.com/activity/Silver/part-00001-7ff60690-4795-4a62-9176-eb5766958a1b-c000.snappy.parquet&#39;, name=&#39;part-00001-7ff60690-4795-4a62-9176-eb5766958a1b-c000.snappy.parquet&#39;, size=1170933),\n FileInfo(path=&#39;dbfs:/user/lregoazd@outlook.com/activity/Silver/part-00002-a83f81cc-3f65-42a6-99b0-b45495c71bcb-c000.snappy.parquet&#39;, name=&#39;part-00002-a83f81cc-3f65-42a6-99b0-b45495c71bcb-c000.snappy.parquet&#39;, size=1110853),\n FileInfo(path=&#39;dbfs:/user/lregoazd@outlook.com/activity/Silver/part-00003-b9006580-0dbb-4c34-8d06-657da66d8ab3-c000.snappy.parquet&#39;, name=&#39;part-00003-b9006580-0dbb-4c34-8d06-657da66d8ab3-c000.snappy.parquet&#39;, size=949320)]</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["-sandbox\n#### See list of active streams.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You should currently see two active streams, one for each streaming write that you've triggered. If you have called `display` on either of your streaming DataFrames, you will see an additional stream, as `display` writes the stream to memory."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  print(s.id)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">b05df5b8-db01-48fd-bcd6-bf00d326211d\n63c93ed9-f2ee-4ad6-a7d2-c54c61f17a3c\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["-sandbox\n### Gold Table: Grouped Count of Events\n\nHere we read a stream of data from `activitySilverPath` and write another stream to `activityGoldPath/groupedCount`.\n\nThe data consists of a total counts of all event, grouped by `hour`, `gt`, and `countryCode3`.\n\nPerforming this aggregation allows us to reduce the total number of rows in our table from hundreds of thousands (or millions, once we've loaded our batch data) to dozens.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice that we're writing to a named directory within our gold path. If we wish to define additional aggregations, we would organize these parallel to thie directory to avoid metadata write conflicts."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import window, hour\n\n(spark.readStream\n  .format(\"delta\")\n  .load(activitySilverPath)\n  .groupBy(window(\"Arrival_Time\", \"60 minute\"),\"gt\", \"countryCode3\")\n  .count()\n  .withColumn(\"hour\",hour(col(\"window.start\")))\n  .drop(\"window\")\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", groupedCountCheckpoint)\n  .outputMode(\"complete\")\n  .start(groupedCountPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f6df80657d0&gt;</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["### CREATE A Table Using Delta Lake\n\nCreate a table called `gt_count` using `DELTA` out of the above data.\n\nNOTE: You will not be able to run this command until the `activityCountsQuery` has initialized."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  DROP TABLE IF EXISTS grouped_count\n\"\"\")\nspark.sql(\"\"\"\n  CREATE TABLE grouped_count\n  USING DELTA\n  LOCATION '{}'\n\"\"\".format(groupedCountPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: DataFrame[]</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["-sandbox\n#### Important Considerations for `complete` Output with Delta\n\nWhen using `complete` output mode, we rewrite the entire state of our table each time our logic runs. While this is ideal for calculating aggregates, we **cannot** read a stream from this directory, as Structured Streaming assumes data is only being appended in the upstream logic.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Certain options can be set to change this behavior, but have other limitations attached. For more details, refer to [Delta Streaming: Ignoring Updates and Deletes](https://docs.databricks.com/delta/delta-streaming.html#ignoring-updates-and-deletes).\n\nThe gold Delta table we have just registered will perform a static read of the current state of the data each time we run the following query."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM grouped_count"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>gt</th><th>countryCode3</th><th>count</th><th>hour</th></tr></thead><tbody><tr><td>stairsdown</td><td>IND</td><td>440</td><td>12</td></tr><tr><td>stairsdown</td><td>DEU</td><td>2542</td><td>13</td></tr><tr><td>stairsdown</td><td>NGA</td><td>2810</td><td>11</td></tr><tr><td>stairsdown</td><td>USA</td><td>2468</td><td>12</td></tr><tr><td>stairsdown</td><td>AUS</td><td>2461</td><td>13</td></tr><tr><td>stairsdown</td><td>IND</td><td>1752</td><td>13</td></tr><tr><td>stairsdown</td><td>FRA</td><td>2650</td><td>14</td></tr><tr><td>stairsdown</td><td>BRA</td><td>2521</td><td>14</td></tr><tr><td>bike</td><td>NGA</td><td>3204</td><td>11</td></tr><tr><td>sit</td><td>NGA</td><td>2970</td><td>11</td></tr><tr><td>stairsup</td><td>IND</td><td>1900</td><td>13</td></tr><tr><td>stairsup</td><td>NGA</td><td>2944</td><td>11</td></tr><tr><td>stairsup</td><td>USA</td><td>2651</td><td>12</td></tr><tr><td>stairsup</td><td>IND</td><td>714</td><td>12</td></tr><tr><td>stairsup</td><td>DZA</td><td>2754</td><td>12</td></tr><tr><td>stairsup</td><td>FRA</td><td>2601</td><td>14</td></tr><tr><td>stairsup</td><td>BRA</td><td>2654</td><td>14</td></tr><tr><td>stairsup</td><td>CHN</td><td>2255</td><td>10</td></tr><tr><td>null</td><td>BRA</td><td>4110</td><td>14</td></tr><tr><td>null</td><td>IND</td><td>685</td><td>13</td></tr><tr><td>sit</td><td>CHN</td><td>3077</td><td>10</td></tr><tr><td>sit</td><td>AUS</td><td>3050</td><td>13</td></tr><tr><td>stand</td><td>DZA</td><td>2223</td><td>11</td></tr><tr><td>stand</td><td>FRA</td><td>2684</td><td>13</td></tr><tr><td>stand</td><td>DEU</td><td>2742</td><td>13</td></tr><tr><td>stand</td><td>AUS</td><td>2942</td><td>13</td></tr><tr><td>stand</td><td>IND</td><td>2786</td><td>12</td></tr><tr><td>stand</td><td>USA</td><td>2941</td><td>12</td></tr><tr><td>stand</td><td>NGA</td><td>2835</td><td>11</td></tr><tr><td>stand</td><td>DZA</td><td>810</td><td>12</td></tr><tr><td>stand</td><td>CHN</td><td>3010</td><td>10</td></tr><tr><td>walk</td><td>DZA</td><td>3074</td><td>12</td></tr><tr><td>stairsdown</td><td>CHN</td><td>2545</td><td>10</td></tr><tr><td>sit</td><td>FRA</td><td>3062</td><td>13</td></tr><tr><td>stairsdown</td><td>DZA</td><td>2538</td><td>12</td></tr><tr><td>bike</td><td>DZA</td><td>3229</td><td>12</td></tr><tr><td>walk</td><td>FRA</td><td>1721</td><td>13</td></tr><tr><td>null</td><td>AUS</td><td>1934</td><td>13</td></tr><tr><td>null</td><td>IND</td><td>851</td><td>12</td></tr><tr><td>null</td><td>DZA</td><td>3180</td><td>12</td></tr><tr><td>stairsup</td><td>DEU</td><td>2853</td><td>13</td></tr><tr><td>stand</td><td>BRA</td><td>2924</td><td>14</td></tr><tr><td>bike</td><td>AUS</td><td>2654</td><td>13</td></tr><tr><td>walk</td><td>IND</td><td>3127</td><td>12</td></tr><tr><td>null</td><td>NGA</td><td>2837</td><td>11</td></tr><tr><td>null</td><td>USA</td><td>1725</td><td>12</td></tr><tr><td>walk</td><td>DEU</td><td>3293</td><td>13</td></tr><tr><td>null</td><td>DEU</td><td>2206</td><td>13</td></tr><tr><td>null</td><td>FRA</td><td>815</td><td>13</td></tr><tr><td>walk</td><td>NGA</td><td>3122</td><td>11</td></tr><tr><td>bike</td><td>USA</td><td>2269</td><td>12</td></tr><tr><td>bike</td><td>CHN</td><td>2833</td><td>10</td></tr><tr><td>bike</td><td>DEU</td><td>3770</td><td>14</td></tr><tr><td>null</td><td>DEU</td><td>958</td><td>14</td></tr><tr><td>walk</td><td>CHN</td><td>3423</td><td>10</td></tr><tr><td>walk</td><td>FRA</td><td>931</td><td>14</td></tr><tr><td>bike</td><td>IND</td><td>2456</td><td>13</td></tr><tr><td>null</td><td>FRA</td><td>816</td><td>14</td></tr><tr><td>walk</td><td>BRA</td><td>3237</td><td>14</td></tr><tr><td>bike</td><td>FRA</td><td>2633</td><td>14</td></tr><tr><td>null</td><td>CHN</td><td>4420</td><td>10</td></tr><tr><td>walk</td><td>USA</td><td>2947</td><td>12</td></tr><tr><td>stairsup</td><td>AUS</td><td>2250</td><td>13</td></tr><tr><td>sit</td><td>IND</td><td>2728</td><td>12</td></tr><tr><td>sit</td><td>DZA</td><td>2900</td><td>12</td></tr><tr><td>sit</td><td>USA</td><td>2801</td><td>12</td></tr><tr><td>sit</td><td>BRA</td><td>2821</td><td>14</td></tr><tr><td>walk</td><td>AUS</td><td>2698</td><td>13</td></tr><tr><td>bike</td><td>BRA</td><td>3421</td><td>14</td></tr><tr><td>sit</td><td>DEU</td><td>2603</td><td>13</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["-sandbox\n### Materialized View: Windowed Count of Hourly `gt` Events\n\nPlot the occurrence of all events grouped by `gt`.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Because we're using `complete` output mode for our gold table write, we cannot define a streaming plot on these files.\n\nInstead, we'll define a temp table based on the files written to our silver table. We will them use this table to execute our streaming queries.\n\nIn order to create a LIVE bar chart of the data, you'll need to fill out the <b>Plot Options</b> as shown:\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/ch5-plot-options.png\"/></div><br/>\n\n### Note on Gold Tables & Materialized Views\n\nWhen we call `display` on a streaming DataFrame or execute a SQL query on a streaming view, we are using memory as our sink. \n\nIn this case, we have already calculated all the values necessary to materialize our streaming view above in the gold table we've written to disk. \n\n**However**, we re-execute this logic on our silver table to generate streaming views, as structured streaming will not support reads from upstream files that have beem overwritten."],"metadata":{}},{"cell_type":"code","source":["(spark.readStream\n  .format(\"delta\")\n  .load(activitySilverPath)\n  .createOrReplaceTempView(\"query_table\")\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["%sql\nSELECT gt, HOUR(Arrival_Time) hour, COUNT(*) total_events\nFROM query_table\nGROUP BY gt, HOUR(Arrival_Time)\nORDER BY hour"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>gt</th><th>hour</th><th>total_events</th></tr></thead><tbody><tr><td>null</td><td>10</td><td>140087</td></tr><tr><td>bike</td><td>10</td><td>88920</td></tr><tr><td>sit</td><td>10</td><td>115241</td></tr><tr><td>walk</td><td>10</td><td>126856</td></tr><tr><td>stand</td><td>10</td><td>105632</td></tr><tr><td>stairsdown</td><td>10</td><td>79414</td></tr><tr><td>stairsup</td><td>10</td><td>77237</td></tr><tr><td>bike</td><td>11</td><td>105874</td></tr><tr><td>null</td><td>11</td><td>100736</td></tr><tr><td>sit</td><td>11</td><td>117042</td></tr><tr><td>stairsdown</td><td>11</td><td>91944</td></tr><tr><td>walk</td><td>11</td><td>123285</td></tr><tr><td>stairsup</td><td>11</td><td>96378</td></tr><tr><td>stand</td><td>11</td><td>183771</td></tr><tr><td>sit</td><td>12</td><td>313831</td></tr><tr><td>stand</td><td>12</td><td>227357</td></tr><tr><td>stairsup</td><td>12</td><td>225229</td></tr><tr><td>stairsdown</td><td>12</td><td>183237</td></tr><tr><td>null</td><td>12</td><td>203751</td></tr><tr><td>bike</td><td>12</td><td>176923</td></tr><tr><td>walk</td><td>12</td><td>354825</td></tr><tr><td>sit</td><td>13</td><td>327053</td></tr><tr><td>stairsdown</td><td>13</td><td>229347</td></tr><tr><td>stairsup</td><td>13</td><td>258292</td></tr><tr><td>walk</td><td>13</td><td>293132</td></tr><tr><td>stand</td><td>13</td><td>286641</td></tr><tr><td>bike</td><td>13</td><td>164880</td></tr><tr><td>null</td><td>13</td><td>190834</td></tr><tr><td>walk</td><td>14</td><td>162304</td></tr><tr><td>stairsup</td><td>14</td><td>179462</td></tr><tr><td>stairsdown</td><td>14</td><td>165117</td></tr><tr><td>bike</td><td>14</td><td>327112</td></tr><tr><td>null</td><td>14</td><td>200317</td></tr><tr><td>stand</td><td>14</td><td>107382</td></tr><tr><td>sit</td><td>14</td><td>111547</td></tr></tbody></table></div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["## Batch Load Data into Bronze Table\n\nWe can use the same pipeline to process batch data.\n\nBy loading our raw data into our bronze table, we will push it through our already running streaming logic.\n\nHere, we'll run 4 batches of around 170k records. We can track each batch through our streaming plots above."],"metadata":{}},{"cell_type":"code","source":["for batch in range(4):\n  (spark\n    .read\n    .format(\"json\")\n    .schema(schema)\n    .load(\"/mnt/training/definitive-guide/data/activity-json/batch-{}\".format(batch))\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .save(activityBronzePath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["Note that even on our small cluster, we can pass a batch of over 5 million records through our logic above without problems."],"metadata":{}},{"cell_type":"code","source":["(spark\n  .read\n  .format(\"json\")\n  .schema(schema)\n  .load(\"/mnt/training/definitive-guide/data/activity-json/batch\")\n  .write\n  .format(\"delta\")\n  .mode(\"append\")\n  .save(activityBronzePath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["-sandbox\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> While our streaming materialized view above updates as data flows in, we can also easily generate this view from our `grouped_count` table. \n\nWe will need to re-run this query each time we wish to update the data. Run the below query now, and then after your batch has finished processing.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> The state reflected in a query on a registered Delta table will always reflect the most recent valid state of the files."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM grouped_count"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>gt</th><th>countryCode3</th><th>count</th><th>hour</th></tr></thead><tbody><tr><td>stairsdown</td><td>IND</td><td>440</td><td>12</td></tr><tr><td>stairsdown</td><td>DEU</td><td>2542</td><td>13</td></tr><tr><td>stairsdown</td><td>NGA</td><td>2810</td><td>11</td></tr><tr><td>stairsdown</td><td>USA</td><td>2468</td><td>12</td></tr><tr><td>stairsdown</td><td>AUS</td><td>2461</td><td>13</td></tr><tr><td>stairsdown</td><td>IND</td><td>1752</td><td>13</td></tr><tr><td>stairsdown</td><td>FRA</td><td>2650</td><td>14</td></tr><tr><td>stairsdown</td><td>BRA</td><td>2521</td><td>14</td></tr><tr><td>sit</td><td>NGA</td><td>2970</td><td>11</td></tr><tr><td>bike</td><td>NGA</td><td>3204</td><td>11</td></tr><tr><td>stairsup</td><td>IND</td><td>1900</td><td>13</td></tr><tr><td>stairsup</td><td>NGA</td><td>2944</td><td>11</td></tr><tr><td>stairsup</td><td>USA</td><td>2651</td><td>12</td></tr><tr><td>stairsup</td><td>IND</td><td>714</td><td>12</td></tr><tr><td>stairsup</td><td>DZA</td><td>2754</td><td>12</td></tr><tr><td>stairsup</td><td>FRA</td><td>2601</td><td>14</td></tr><tr><td>stairsup</td><td>CHN</td><td>77237</td><td>10</td></tr><tr><td>stairsup</td><td>BRA</td><td>2654</td><td>14</td></tr><tr><td>null</td><td>BRA</td><td>4110</td><td>14</td></tr><tr><td>null</td><td>IND</td><td>685</td><td>13</td></tr><tr><td>sit</td><td>CHN</td><td>115241</td><td>10</td></tr><tr><td>sit</td><td>AUS</td><td>3050</td><td>13</td></tr><tr><td>stand</td><td>DZA</td><td>2223</td><td>11</td></tr><tr><td>stand</td><td>FRA</td><td>2684</td><td>13</td></tr><tr><td>stand</td><td>DEU</td><td>2742</td><td>13</td></tr><tr><td>stand</td><td>AUS</td><td>2942</td><td>13</td></tr><tr><td>stand</td><td>IND</td><td>2786</td><td>12</td></tr><tr><td>stand</td><td>USA</td><td>2941</td><td>12</td></tr><tr><td>stand</td><td>NGA</td><td>2835</td><td>11</td></tr><tr><td>stand</td><td>DZA</td><td>810</td><td>12</td></tr><tr><td>stand</td><td>CHN</td><td>105632</td><td>10</td></tr><tr><td>walk</td><td>DZA</td><td>3074</td><td>12</td></tr><tr><td>stairsdown</td><td>CHN</td><td>79414</td><td>10</td></tr><tr><td>sit</td><td>FRA</td><td>3062</td><td>13</td></tr><tr><td>stairsdown</td><td>DZA</td><td>2538</td><td>12</td></tr><tr><td>bike</td><td>DZA</td><td>3229</td><td>12</td></tr><tr><td>walk</td><td>FRA</td><td>1721</td><td>13</td></tr><tr><td>null</td><td>AUS</td><td>1934</td><td>13</td></tr><tr><td>null</td><td>IND</td><td>851</td><td>12</td></tr><tr><td>null</td><td>DZA</td><td>3180</td><td>12</td></tr><tr><td>stairsup</td><td>DEU</td><td>2853</td><td>13</td></tr><tr><td>stand</td><td>BRA</td><td>2924</td><td>14</td></tr><tr><td>bike</td><td>AUS</td><td>2654</td><td>13</td></tr><tr><td>walk</td><td>IND</td><td>3127</td><td>12</td></tr><tr><td>null</td><td>NGA</td><td>2837</td><td>11</td></tr><tr><td>null</td><td>USA</td><td>1725</td><td>12</td></tr><tr><td>walk</td><td>DEU</td><td>3293</td><td>13</td></tr><tr><td>null</td><td>DEU</td><td>2206</td><td>13</td></tr><tr><td>null</td><td>FRA</td><td>815</td><td>13</td></tr><tr><td>walk</td><td>NGA</td><td>3122</td><td>11</td></tr><tr><td>bike</td><td>USA</td><td>2269</td><td>12</td></tr><tr><td>bike</td><td>CHN</td><td>50895</td><td>10</td></tr><tr><td>bike</td><td>DEU</td><td>3770</td><td>14</td></tr><tr><td>null</td><td>DEU</td><td>958</td><td>14</td></tr><tr><td>walk</td><td>CHN</td><td>126856</td><td>10</td></tr><tr><td>walk</td><td>FRA</td><td>931</td><td>14</td></tr><tr><td>bike</td><td>IND</td><td>2456</td><td>13</td></tr><tr><td>null</td><td>FRA</td><td>816</td><td>14</td></tr><tr><td>null</td><td>CHN</td><td>140087</td><td>10</td></tr><tr><td>walk</td><td>BRA</td><td>3237</td><td>14</td></tr><tr><td>bike</td><td>FRA</td><td>2633</td><td>14</td></tr><tr><td>walk</td><td>USA</td><td>2947</td><td>12</td></tr><tr><td>stairsup</td><td>AUS</td><td>2250</td><td>13</td></tr><tr><td>sit</td><td>IND</td><td>2728</td><td>12</td></tr><tr><td>sit</td><td>DZA</td><td>2900</td><td>12</td></tr><tr><td>sit</td><td>USA</td><td>2801</td><td>12</td></tr><tr><td>sit</td><td>BRA</td><td>2821</td><td>14</td></tr><tr><td>walk</td><td>AUS</td><td>2698</td><td>13</td></tr><tr><td>sit</td><td>DEU</td><td>2603</td><td>13</td></tr><tr><td>bike</td><td>BRA</td><td>3421</td><td>14</td></tr></tbody></table></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["## Wrapping Up\n\nFinally, make sure all streams are stopped."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n    s.stop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1947196994887737&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> s <span class=\"ansi-green-fg\">in</span> spark<span class=\"ansi-blue-fg\">.</span>streams<span class=\"ansi-blue-fg\">.</span>active<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">     </span>s<span class=\"ansi-blue-fg\">.</span>stop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/streaming.py</span> in <span class=\"ansi-cyan-fg\">stop</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    151</span>         &#34;&#34;&#34;Stop this streaming query.\n<span class=\"ansi-green-intense-fg ansi-bold\">    152</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 153</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jsq<span class=\"ansi-blue-fg\">.</span>stop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    154</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    155</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.1</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 128</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    130</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o979.stop.\n: java.util.concurrent.TimeoutException: Stream Execution thread for stream display_query_1 [id = 47de691f-c125-469a-bcfd-e03445c95bcc, runId = 6377542a-6c27-4976-93a9-3fc3175c53ad] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.interruptAndAwaitExecutionThreadTermination(StreamExecution.scala:488)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.stop(MicroBatchExecution.scala:172)\n\tat org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.stop(StreamingQueryWrapper.scala:67)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: The stream thread was last executing:\n\tat java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat sun.security.ssl.InputRecord.readFully(InputRecord.java:465)\n\tat sun.security.ssl.InputRecord.read(InputRecord.java:503)\n\tat sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:990)\n\tat sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:948)\n\tat sun.security.ssl.AppInputStream.read(AppInputStream.java:105)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n\tat sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735)\n\tat sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1593)\n\tat sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:92)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1490)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1488)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1487)\n\tat java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:352)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:115)\n\tat hadoop_azure_shaded.com.microsoft.azure.storage.blob.CloudBlob.delete(CloudBlob.java:1054)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.delete(StorageInterfaceImpl.java:313)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.safeDelete(AzureNativeFileSystemStore.java:2625)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.delete(AzureNativeFileSystemStore.java:2663)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.delete(AzureNativeFileSystemStore.java:2681)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.deleteFile(NativeAzureFileSystem.java:2218)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem$1.execute(NativeAzureFileSystem.java:2169)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor.executeParallel(AzureFileSystemThreadPoolExecutor.java:223)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:2179)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1874)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$delete$3(DatabricksFileSystemV2.scala:692)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$Lambda$5305/251904148.apply$mcZ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$delete$2(DatabricksFileSystemV2.scala:690)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$Lambda$5304/2058960702.apply$mcZ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$withUserContextRecorded$2(DatabricksFileSystemV2.scala:939)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$Lambda$2726/1291180441.apply(Unknown Source)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat com.databricks.logging.UsageLogging$$Lambda$300/929307781.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withUserContextRecorded(DatabricksFileSystemV2.scala:912)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$delete$1(DatabricksFileSystemV2.scala:688)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$Lambda$5303/1933046825.apply$mcZ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging$$Lambda$314/411006734.apply(Unknown Source)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat com.databricks.logging.UsageLogging$$Lambda$300/929307781.apply(Unknown Source)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:337)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.delete(DatabricksFileSystemV2.scala:688)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.delete(DatabricksFileSystem.scala:142)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$2(StreamExecution.scala:406)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$Lambda$7866/1722887009.apply(Unknown Source)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:375)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:259)\n</div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["## Summary\n\nDelta Lake is ideally suited for use in streaming data lake contexts.\n\nUse the Delta Lake architecture to craft raw, query, and summary tables to produce beautiful visualizations of key business metrics."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-streaming.html#as-a-sink\" target=\"_blank\">Delta Streaming Write Notation</a>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tagatha Das. This is an excellent video describing how Structured Streaming works.\n* <a href=\"http://lambda-architecture.net/#\" target=\"_blank\">Lambda Architecture</a>\n* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n* <a href=\"https://people.apache.org//~pwendell/spark-nightly/spark-branch-2.1-docs/latest/structured-streaming-kafka-integration.html#\" target=\"_blank\">Reading structured streams from Kafka</a>\n* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-stream#\" target=\"_blank\">Create a Kafka Source Stream</a>\n* <a href=\"https://docs.databricks.com/delta/delta-intro.html#case-study-multi-hop-pipelines#\" target=\"_blank\">Multi Hop Pipelines</a>"],"metadata":{}}],"metadata":{"name":"1-Delta-Architecture","notebookId":1947196994887697},"nbformat":4,"nbformat_minor":0}
