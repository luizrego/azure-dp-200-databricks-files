{"cells":[{"cell_type":"markdown","source":["# Parameterizing Notebooks\n\nThe [Databricks Utilities module](https://docs.databricks.com/dev-tools/databricks-utils.html) includes a number of methods to make notebooks more extensible and easier to take to production. This notebook is designed to be scheduled as a job, but can also be run interactively.\n\n### Learning Objectives\n- Pass parameters to notebooks using widgets\n- Return values from notebooks using exit value"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Widgets\n\nThe `widgets` submodule includes a number of methods to allow interactive variables to be set while working with notebooks in the workspace with an interactive cluster. To learn more about this functionality, refer to the [Databricks documentation](https://docs.databricks.com/notebooks/widgets.html#widgets).\n\nThis notebook will focus on only two of these methods, emphasizing their utility when running a notebook as a job:\n1. `dbutils.widgets.text` accepts a parameter name and a default value. This is the method through which external values can be passed into scheduled notebooks.\n1. `dbutils.widgets.get` accepts a parameter name and retrieves the associated value from the widget with that parameter name."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nIn the cell below, a text widget is created with the default value `\"notebook\"`. This widget expects values to be passed as strings.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If you run this cell in an interactive notebook, you will see the widget populated with the default value at the top of the notebook. This can be manually manipulated."],"metadata":{}},{"cell_type":"code","source":["%scala\ndbutils.widgets.text(\"ranBy\", \"notebook\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["-sandbox\nThe cell below retrieves the value currently associated with the widget and assigns it to a variable. Remember that this value will be passed as a string--be sure to cast it to the correct type if you wish to pass numeric values or use JSON to pass multiple fields.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If no parameter is passed to the notebook when scheduling, the default value will be used."],"metadata":{}},{"cell_type":"code","source":["%scala\nval ranBy = dbutils.widgets.get(\"ranBy\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ranBy: String = notebook\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Taken together, `dbutils.widgets.text` allows the passing of external values and `dbutils.widgets.get` allows those values to be referenced."],"metadata":{}},{"cell_type":"markdown","source":["## Parameterized Logic\nThe following code block writes a simple file that records the time the notebook was run and the value associated with the `\"ranBy\"` parameter/widget. The final line displays the full content of this file from all previous executions by the present user."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{lit, unix_timestamp}\nimport org.apache.spark.sql.types.TimestampType\n\nval tags = com.databricks.logging.AttributionContext.current.tags\nval username = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\nval path = username+\"/runLog\"\n\nspark\n  .range(1)\n  .select(unix_timestamp.alias(\"runtime\").cast(TimestampType), lit(ranBy).alias(\"ranBy\"))\n  .write\n  .mode(\"APPEND\")\n  .parquet(path)\n\ndisplay(spark.read.parquet(path))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat com.databricks.common.path.AbstractPath$.fromHadoopPath(AbstractPath.scala:114)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.resolve(DatabricksFileSystemV2.scala:80)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:760)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:430)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:411)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:337)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1439)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:355)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:839)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:817)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1947196994887888:15)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1947196994887888:70)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1947196994887888:72)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$$iw$$iw$$iw.&lt;init&gt;(command-1947196994887888:74)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$$iw$$iw.&lt;init&gt;(command-1947196994887888:76)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$$iw.&lt;init&gt;(command-1947196994887888:78)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read.&lt;init&gt;(command-1947196994887888:80)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$.&lt;init&gt;(command-1947196994887888:84)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$read$.&lt;clinit&gt;(command-1947196994887888)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$eval$.$print(&lt;notebook&gt;:6)\n\tat linef33a04f525764c139ae2047cfb6ca68737.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:202)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:396)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Exit Value\nThe `notebook` submodule contains only two methods. [Documentation here](https://docs.databricks.com/notebooks/notebook-workflows.html#notebook-workflows).\n1. `dbutils.notebook.run` allows you to call another notebook using a relative path.\n1. `dbutils.notebook.exit` allows you to return an exit value that can be captured and referenced by integrated scheduling services and APIs. While running in interactive mode, this is essentially a no-op as this value does not go anywhere."],"metadata":{}},{"cell_type":"markdown","source":["In the cell below, the value associated with the variable `path` is returned as the exit value."],"metadata":{}},{"cell_type":"code","source":["%scala\ndbutils.notebook.exit(path)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"Record-Run","notebookId":1947196994887870},"nbformat":4,"nbformat_minor":0}
