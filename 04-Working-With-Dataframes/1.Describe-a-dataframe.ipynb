{"cells":[{"cell_type":"markdown","source":["# Describe a DataFrame\n\nYour data processing in Azure Databricks is accomplished by defining Dataframes to read and process the Data.\n\nThis notebook will introduce how to read your data using Azure Databricks Dataframes."],"metadata":{}},{"cell_type":"markdown","source":["#Introduction\n\n** Data Source **\n* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n* Size on Disk: ~23 MB\n* Type: Compressed Parquet File\n* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n\n**Technical Accomplishments:**\n* Develop familiarity with the `DataFrame` APIs\n* Introduce the classes...\n  * `SparkSession`\n  * `DataFrame` (aka `Dataset[Row]`)\n* Introduce the actions...\n  * `count()`"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n\n* In this notebook, we will be using a compressed parquet \"file\" called **pagecounts** (~23 MB file from Wikipedia)\n* We will explore the data and develop an understanding of it as we progress.\n* You can read more about this dataset here: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw/\" target=\"_blank\">Page view statistics for Wikimedia projects</a>.\n\nWe can use **dbutils.fs.ls()** to view our data on the DBFS."],"metadata":{}},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\n\nspark.conf.set(sasEntity, sasToken)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["path = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\nfiles = dbutils.fs.ls(path)\ndisplay(files)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["As we can see from the files listed above, this data is stored in <a href=\"https://parquet.apache.org\" target=\"_blank\">Parquet</a> files which can be read in a single command, the result of which will be a `DataFrame`."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Create a DataFrame\n* We can read the Parquet files into a `DataFrame`.\n* We'll start with the object **spark**, an instance of `SparkSession` and the entry point to Spark 2.0 applications.\n* From there we can access the `read` object which gives us an instance of `DataFrameReader`."],"metadata":{}},{"cell_type":"code","source":["parquetDir = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n  .read                     # Our DataFrameReader\n  .parquet(parquetDir)      # Returns an instance of DataFrame\n)\nprint(pagecountsEnAllDF)    # Python hack to see the data type"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) count()\n\nIf you look at the API docs, `count()` is described like this:\n> Returns the number of rows in the Dataset.\n\n`count()` will trigger a job to process the request and return a value.\n\nWe can now count all records in our `DataFrame` like this:"],"metadata":{}},{"cell_type":"code","source":["total = pagecountsEnAllDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["That tells us that there are around 2 million rows in the `DataFrame`."],"metadata":{}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Use common DataFrame methods]($./2.Use-common-dataframe-methods)"],"metadata":{}}],"metadata":{"name":"1.Describe-a-dataframe","notebookId":1947196994889243},"nbformat":4,"nbformat_minor":0}
