{"cells":[{"cell_type":"markdown","source":["# Use common DataFrame methods\n\nIn the previous notebook, you ended off by executing a count of records in a DataFrame. We will now build upon that concept by introducing common DataFrame methods."],"metadata":{}},{"cell_type":"markdown","source":["**Technical Accomplishments:**\n* Develop familiarity with the `DataFrame` APIs\n* Use common DataFrame methods for performance\n* Explore the Spark API documentation"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Prepare the data source."],"metadata":{}},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\n\nspark.conf.set(sasEntity, sasToken)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Create the DataFrame. This is the same one we created in the previous notebook."],"metadata":{}},{"cell_type":"code","source":["parquetDir = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n  .read                     # Our DataFrameReader\n  .parquet(parquetDir)      # Returns an instance of DataFrame\n)\nprint(pagecountsEnAllDF)    # Python hack to see the data type"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Execute a count on the DataFrame as we did at the end of the previous notebook."],"metadata":{}},{"cell_type":"code","source":["total = pagecountsEnAllDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["That tells us that there are around 2 million rows in the `DataFrame`. \n\nBefore we take a closer look at the contents of the `DataFrame`, let us introduce a technique that speeds up processing."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) cache() & persist()\n\nThe ability to cache data is one technique for achieving better performance with Apache Spark. \n\nThis is because every action requires Spark to read the data from its source (Azure Blob, Amazon S3, HDFS, etc.) but caching moves that data into the memory of the local executor for \"instant\" access.\n\n`cache()` is just an alias for `persist()`."],"metadata":{}},{"cell_type":"code","source":["(pagecountsEnAllDF\n  .cache()         # Mark the DataFrame as cached\n  .count()         # Materialize the cache\n) "],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["If you re-run that command, it should take significantly less time."],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Performance considerations of Caching Data\n\nWhen Caching Data you are placing it on the workers of the cluster. \n\nCaching takes resources, before moving a notebook into production please check and verify that you are appropriately using cache."],"metadata":{}},{"cell_type":"markdown","source":["And as a quick side note, you can remove a cache by calling the `DataFrame`'s `unpersist()` method but, it is not necessary."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Our Data\n\nLet's continue by taking a look at the type of data we have. \n\nWe can do this with the `printSchema()` command:"],"metadata":{}},{"cell_type":"code","source":["pagecountsEnAllDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["We should now be able to see that we have four columns of data:\n* **project** (*string*): The name of the Wikipedia project. This will include values such as:\n  * **en**: The English version of Wikipedia.\n  * **fr**: The French version of Wikipedia.\n  * **en.d**: The English version of Wiktionary.\n  * **fr.b**: The French version of Wikibooks.\n  * **de.n**: The German version of Wikinews.\n* **article** (*string*): The name of the article in the corresponding project. This will include values such as:\n  * <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\" target=\"_blank\">Apache_Spark</a>\n  * <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei_Zaharia</a>\n  * <a href=\"https://en.wikipedia.org/wiki/Kevin_Bacon\" target=\"_blank\">Kevin_Bacon</a>\n* **requests** (*integer*): The number of requests (clicks) the article has received in the hour this data represents.\n* **bytes_served** (*long*): The total number of bytes delivered for the requested article.\n  * **Note:** In our copy of the data, this value is zero for all records and consequently is of no value to us."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Spark API\n\nYou have already seen one command available to the `DataFrame` class, namely `DataFrame.printSchema()`\n  \nLet's take a look at the API to see what other operations we have available."],"metadata":{}},{"cell_type":"markdown","source":["### **Spark API Home Page**\n0. Open a new browser tab\n0. Google for **Spark API Latest** or **Spark API _x.x.x_** for a specific version.\n0. Select **Spark API Documentation - Spark _x.x.x_ Documentation - Apache Spark** \n\nOther Documentation:\n* Programming Guides for DataFrames, SQL, Graphs, Machine Learning, Streaming...\n* Deployment Guides for Spark Standalone, Mesos, Yarn...\n* Configuration, Monitoring, Tuning, Security...\n\nHere are some shortcuts\n  * <a href=\"https://spark.apache.org/docs/latest/api.html\" target=\"_blank\">Spark API Documentation - Latest</a>\n  * <a href=\"https://spark.apache.org/docs/2.1.1/api.html\" target=\"_blank\">Spark API Documentation - 2.1.1</a>\n  * <a href=\"https://spark.apache.org/docs/2.1.0/api.html\" target=\"_blank\">Spark API Documentation - 2.1.0</a>\n  * <a href=\"https://spark.apache.org/docs/2.0.2/api.html\" target=\"_blank\">Spark API Documentation - 2.0.2</a>\n  * <a href=\"https://spark.apache.org/docs/1.6.3/api.html\" target=\"_blank\">Spark API Documentation - 1.6.3</a>"],"metadata":{}},{"cell_type":"markdown","source":["Naturally, which set of documentation you will use depends on which language you will use."],"metadata":{}},{"cell_type":"markdown","source":["### Spark API (Python)\n\n0. Select **Spark Python API (Sphinx)**.\n0. Look up the documentation for `pyspark.sql.DataFrame`.\n  0. In the lower-left-hand-corner type **DataFrame** into the search field.\n  0. Hit **[Enter]**.\n  0. The search results should appear in the right-hand pane.\n  0. Click on **pyspark.sql.DataFrame (Python class, in pyspark.sql module)**\n  0. The documentation should open in the right-hand pane."],"metadata":{}},{"cell_type":"markdown","source":["### Spark API (Scala)\n\n0. Select **Spark Scala API (Scaladoc)**.\n0. Look up the documentation for `org.apache.spark.sql.DataFrame`.\n  0. In the upper-left-hand-corner type **DataFrame** into the search field.\n  0. The search will execute automatically.\n  0. In the class/package list, click on **DataFrame**.\n  0. The documentation should open in the right-hand pane.\n  \nThis isn't going to work, but why?"],"metadata":{}},{"cell_type":"markdown","source":["### Spark API (Scala), Try #2\n\nLook up the documentation for `org.apache.spark.sql.Dataset`.\n  0. In the upper-left-hand-corner type **Dataset** into the search field.\n  0. The search will execute automatically.\n  0. In the class/package list, click on **Dataset**.\n  0. The documentation should open in the right-hand pane."],"metadata":{}},{"cell_type":"markdown","source":["Now that we have found the proper documentation, we can take a quick peek at the function `printSchema()`.\n\nNothing special here.\n\nIf you look at the API docs, `printSchema(..)` is described like this:\n> Prints the schema to the console in a nice tree format."],"metadata":{}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Use the Display function]($./3.Display-function)"],"metadata":{}}],"metadata":{"name":"2.Use-common-dataframe-methods","notebookId":1947196994889338},"nbformat":4,"nbformat_minor":0}
