{"cells":[{"cell_type":"markdown","source":["# Reading Data - JSON Files\n\n**Technical Accomplishments:**\n- Read data from:\n  * JSON without a Schema\n  * JSON with a Schema"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run \"./Includes/Utility-Methods\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ InferSchema\n\nReading in JSON isn't that much different than reading in CSV files.\n\nLet's start with taking a look at all the different options that go along with reading in JSON files."],"metadata":{}},{"cell_type":"markdown","source":["### JSON Lines\n\nMuch like the CSV reader, the JSON reader also assumes...\n* That there is one JSON object per line and...\n* That it's delineated by a new-line.\n\nThis format is referred to as **JSON Lines** or **newline-delimited JSON** \n\nMore information about this format can be found at <a href=\"http://jsonlines.org/\" target=\"_blank\">http://jsonlines.org</a>.\n\n** *Note:* ** *Spark 2.2 was released on July 11th 2016. With that comes File IO improvements for CSV & JSON, but more importantly, **Support for parsing multi-line JSON and CSV files**. You can read more about that (and other features in Spark 2.2) in the <a href=\"https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html\" target=\"_blank\">Databricks Blog</a>.*"],"metadata":{}},{"cell_type":"markdown","source":["### The Data Source\n* For this exercise, we will be using the file called **snapshot-2016-05-26.json** (<a href=\"https://wikitech.wikimedia.org/wiki/Stream.wikimedia.org/rc\" target=\"_blank\">4 MB</a> file from Wikipedia).\n* The data represents a set of edits to Wikipedia articles captured in May of 2016.\n* It's located on the DBFS at **dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json**\n* Like we did with the CSV file, we can use **&percnt;fs ls ...** to view the file on the DBFS."],"metadata":{}},{"cell_type":"code","source":["%fs ls dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Like we did with the CSV file, we can use **&percnt;fs head ...** to peek at the first couple lines of the JSON file."],"metadata":{}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Read The JSON File\n\nThe command to read in JSON looks very similar to that of CSV.\n\nIn addition to reading the JSON file, we will also print the resulting schema."],"metadata":{}},{"cell_type":"code","source":["jsonFile = \"dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json\"\n\nwikiEditsDF = (spark.read           # The DataFrameReader\n    .option(\"inferSchema\", \"true\")  # Automatically infer data types & column names\n    .json(jsonFile)                 # Creates a DataFrame from JSON after reading in the file\n )\nwikiEditsDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["With our DataFrame created, we can now take a peak at the data.\n\nBut to demonstrate a unique aspect of JSON data (or any data with embedded fields), we will first create a temporary view and then view the data via SQL:"],"metadata":{}},{"cell_type":"code","source":["# create a view called wiki_edits\nwikiEditsDF.createOrReplaceTempView(\"wiki_edits\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["And now we can take a peak at the data with simple SQL SELECT statement:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM wiki_edits "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Notice the **geocoding** column has embedded data.\n\nYou can expand the fields by clicking the right triangle in each row.\n\nBut we can also reference the sub-fields directly as we see in the following SQL statement:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT channel, page, geocoding.city, geocoding.latitude, geocoding.longitude \nFROM wiki_edits \nWHERE geocoding.city IS NOT NULL"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Review: Reading from JSON w/ InferSchema\n\nWhile there are similarities between reading in CSV & JSON there are some key differences:\n* We only need one job even when inferring the schema.\n* There is no header which is why there isn't a second job in this case - the column names are extracted from the JSON object's attributes.\n* Unlike CSV which reads in 100% of the data, the JSON reader only samples the data.  \n**Note:** In Spark 2.2 the behavior was changed to read in the entire JSON file."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ User-Defined Schema\n\nTo avoid the extra job, we can (just like we did with CSV) specify the schema for the `DataFrame`."],"metadata":{}},{"cell_type":"markdown","source":["### Step #1 - Create the Schema\n\nCompared to our CSV example, the structure of this data is a little more complex.\n\nNote that we can support complex data types as seen in the field `geocoding`."],"metadata":{}},{"cell_type":"code","source":["# Required for StructField, StringType, IntegerType, etc.\nfrom pyspark.sql.types import *\n\njsonSchema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True)\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),\n  StructField(\"page\", StringType(), True),\n  StructField(\"pageURL\", StringType(), True),\n  StructField(\"timestamp\", StringType(), True),\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["That was a lot of typing to get our schema!\n\nFor a small file, manually creating the the schema may not be worth the effort.\n\nHowever, for a large file, the time to manually create the schema may be worth the trade off of a really long infer-schema process."],"metadata":{}},{"cell_type":"markdown","source":["### Step #2 - Read in the JSON\n\nNext, we will read in the JSON file and once again print its schema."],"metadata":{}},{"cell_type":"code","source":["(spark.read            # The DataFrameReader\n  .schema(jsonSchema)  # Use the specified schema\n  .json(jsonFile)      # Creates a DataFrame from JSON after reading in the file\n  .printSchema()\n)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Review: Reading from JSON w/ User-Defined Schema\n* Just like CSV, providing the schema avoids the extra jobs.\n* The schema allows us to rename columns and specify alternate data types.\n* Can get arbitrarily complex in its structure."],"metadata":{}},{"cell_type":"markdown","source":["Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."],"metadata":{}},{"cell_type":"code","source":["jsonDF = (spark.read\n  .schema(jsonSchema)\n  .json(jsonFile)    \n)\nprint(\"Partitions: \" + str(jsonDF.rdd.getNumPartitions()))\nprintRecordsPerPartition(jsonDF)\nprint(\"-\"*80)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["And of course we can view that data here:"],"metadata":{}},{"cell_type":"code","source":["display(jsonDF)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Reading Data - Parquet]($./3.Reading%20Data%20-%20Parquet)"],"metadata":{}}],"metadata":{"name":"2.Reading Data - JSON","notebookId":1947196994889630},"nbformat":4,"nbformat_minor":0}
