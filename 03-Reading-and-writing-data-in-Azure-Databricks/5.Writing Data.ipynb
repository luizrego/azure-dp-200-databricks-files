{"cells":[{"cell_type":"markdown","source":["# Writing Data\n\nJust as there are many ways to read data, we have just as many ways to write data.\n\nIn this notebook, we will take a quick peek at how to write data back out to Parquet files.\n\n**Technical Accomplishments:**\n- Writing data to Parquet files"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Writing Data\n\nLet's start with one of our original CSV data sources, **pageviews_by_second.tsv**:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ncsvSchema = StructType([\n  StructField(\"timestamp\", StringType(), False),\n  StructField(\"site\", StringType(), False),\n  StructField(\"requests\", IntegerType(), False)\n])\n\ncsvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n\ncsvDF = (spark.read\n  .option('header', 'true')\n  .option('sep', \"\\t\")\n  .schema(csvSchema)\n  .csv(csvFile)\n)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Now that we have a `DataFrame`, we can write it back out as Parquet files or other various formats."],"metadata":{}},{"cell_type":"code","source":["fileName = userhome + \"/pageviews_by_second.parquet\"\nprint(\"Output location: \" + fileName)\n\n(csvDF.write                       # Our DataFrameWriter\n  .option(\"compression\", \"snappy\") # One of none, snappy, gzip, and lzo\n  .mode(\"overwrite\")               # Replace existing files\n  .parquet(fileName)               # Write DataFrame to Parquet files\n)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Now that the file has been written out, we can see it in the DBFS:"],"metadata":{}},{"cell_type":"code","source":["display(\n  dbutils.fs.ls(fileName)\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["And lastly we can read that same parquet file back in and display the results:"],"metadata":{}},{"cell_type":"code","source":["display(\n  spark.read.parquet(fileName)\n)\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Reading Data - Lab]($./6.Reading%20Data%20-%20Lab)"],"metadata":{}}],"metadata":{"name":"5.Writing Data","notebookId":1947196994889662},"nbformat":4,"nbformat_minor":0}
