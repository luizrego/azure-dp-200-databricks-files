{"cells":[{"cell_type":"markdown","source":["# Reading Data - Parquet Files\n\n**Technical Accomplishments:**\n- Introduce the Parquet file format.\n- Read data from:\n  - Parquet files without a schema.\n  - Parquet files with a schema."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run \"./Includes/Utility-Methods\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["-sandbox\n<div style=\"float:right; margin-right:1em\">\n  <img src=\"https://parquet.apache.org/assets/img/parquet_logo.png\"><br>\n  <a href=\"https://parquet.apache.org/\" target=\"_blank\">https&#58;//parquet.apache.org</a>\n</div>\n\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from Parquet Files\n\n<strong style=\"font-size:larger\">\"</strong>Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.<strong style=\"font-size:larger\">\"</strong><br>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### About Parquet Files\n* Free & Open Source.\n* Increased query performance over row-based data stores.\n* Provides efficient data compression.\n* Designed for performance on large data sets.\n* Supports limited schema evolution.\n* Is a splittable \"file format\".\n* A <a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Column-Oriented</a> data store\n\n&nbsp;&nbsp;&nbsp;&nbsp;** Row Format ** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Column Format**\n\n<table style=\"border:0\">\n\n  <tr>\n    <th>ID</th><th>Name</th><th>Score</th>\n    <th style=\"border-top:0;border-bottom:0\">&nbsp;</th>\n    <th>ID:</th><td>1</td><td>2</td>\n    <td style=\"border-right: 1px solid #DDDDDD\">3</td>\n  </tr>\n\n  <tr>\n    <td>1</td><td>john</td><td>4.1</td>\n    <td style=\"border-top:0;border-bottom:0\">&nbsp;</td>\n    <th>Name:</th><td>john</td><td>mike</td>\n    <td style=\"border-right: 1px solid #DDDDDD\">sally</td>\n  </tr>\n\n  <tr>\n    <td>2</td><td>mike</td><td>3.5</td>\n    <td style=\"border-top:0;border-bottom:0\">&nbsp;</td>\n    <th style=\"border-bottom: 1px solid #DDDDDD\">Score:</th>\n    <td style=\"border-bottom: 1px solid #DDDDDD\">4.1</td>\n    <td style=\"border-bottom: 1px solid #DDDDDD\">3.5</td>\n    <td style=\"border-bottom: 1px solid #DDDDDD; border-right: 1px solid #DDDDDD\">6.4</td>\n  </tr>\n\n  <tr>\n    <td style=\"border-bottom: 1px solid #DDDDDD\">3</td>\n    <td style=\"border-bottom: 1px solid #DDDDDD\">sally</td>\n    <td style=\"border-bottom: 1px solid #DDDDDD; border-right: 1px solid #DDDDDD\">6.4</td>\n  </tr>\n\n</table>\n\nSee also\n* <a href=\"https://parquet.apache.org/\" target=\"_blank\">https&#58;//parquet.apache.org</a>\n* <a href=\"https://en.wikipedia.org/wiki/Apache_Parquet\" target=\"_blank\">https&#58;//en.wikipedia.org/wiki/Apache_Parquet</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Data Source\n\nThe data for this example shows the number of requests to Wikipedia's mobile and desktop websites (<a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">23 MB</a> from Wikipedia). \n\nThe original file, captured August 5th of 2016 was downloaded, converted to a Parquet file and made available for us at **/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/**"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Unlike our CSV and JSON example, the parquet \"file\" is actually 11 files, 8 of which consist of the bulk of the data and the other three consist of meta-data."],"metadata":{}},{"cell_type":"markdown","source":["### Read in the Parquet Files\n\nTo read in this files, we will specify the location of the parquet directory."],"metadata":{}},{"cell_type":"code","source":["parquetFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/\"\n\n(spark.read              # The DataFrameReader\n  .parquet(parquetFile)  # Creates a DataFrame from Parquet after reading in the file\n  .printSchema()         # Print the DataFrame's schema\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Review: Reading from Parquet Files\n* We do not need to specify the schema - the column names and data types are stored in the parquet files.\n* Only one job is required to **read** that schema from the parquet file's metadata.\n* Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata."],"metadata":{}},{"cell_type":"markdown","source":["### Read in the Parquet Files w/Schema\n\nIf you want to avoid the extra job entirely, we can, again, specify the schema even for parquet files:\n\n** *WARNING* ** *Providing a schema may avoid this one-time hit to determine the `DataFrame's` schema.*  \n*However, if you specify the wrong schema it will conflict with the true schema and will result in an analysis exception at runtime.*"],"metadata":{}},{"cell_type":"code","source":["# Required for StructField, StringType, IntegerType, etc.\nfrom pyspark.sql.types import *\n\nparquetSchema = StructType(\n  [\n    StructField(\"timestamp\", StringType(), False),\n    StructField(\"site\", StringType(), False),\n    StructField(\"requests\", IntegerType(), False)\n  ]\n)\n\n(spark.read               # The DataFrameReader\n  .schema(parquetSchema)  # Use the specified schema\n  .parquet(parquetFile)   # Creates a DataFrame from Parquet after reading in the file\n  .printSchema()          # Print the DataFrame's schema\n)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."],"metadata":{}},{"cell_type":"code","source":["parquetDF = spark.read.schema(parquetSchema).parquet(parquetFile)\n\nprint(\"Partitions: \" + str(parquetDF.rdd.getNumPartitions()) )\nprintRecordsPerPartition(parquetDF)\nprint(\"-\"*80)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["In most/many cases, people do not provide the schema for Parquet files because reading in the schema is such a cheap process.\n\nAnd lastly, let's peek at the data:"],"metadata":{}},{"cell_type":"code","source":["display(parquetDF)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Reading Data - Tables and Views]($./4.Reading%20Data%20-%20Tables%20and%20Views)"],"metadata":{}}],"metadata":{"name":"3.Reading Data - Parquet","notebookId":1947196994889369},"nbformat":4,"nbformat_minor":0}
