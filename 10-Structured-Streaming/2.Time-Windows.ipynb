{"cells":[{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# Working with Time Windows\n\n## In this lesson you:\n* Use sliding windows to aggregate over chunks of data rather than all data\n* Apply watermarking to throw away stale old data that you do not have space to keep\n* Plot live graphs using `display`\n\n## Audience\n* Primary Audience: Data Engineers\n* Secondary Audience: Data Scientists, Software Engineers"],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Getting Started</h2>\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Streaming Aggregations</h2>\n\nContinuous applications often require near real-time decisions on real-time, aggregated statistics.\n\nSome examples include\n* Aggregating errors in data from IoT devices by type\n* Detecting anomalous behavior in a server's log file by aggregating by country.\n* Doing behavior analysis on instant messages via hash tags.\n\nHowever, in the case of streams, you generally don't want to run aggregations over the entire dataset."],"metadata":{}},{"cell_type":"markdown","source":["### What problems might you encounter if you aggregate over a stream's entire dataset?\n\nWhile streams have a definitive start, there conceptually is no end to the flow of data.\n\nBecause there is no \"end\" to a stream, the size of the dataset grows in perpetuity.\n\nThis means that your cluster will eventually run out of resources.\n\nInstead of aggregating over the entire dataset, you can aggregate over data grouped by windows of time (say, every 5 minutes or every hour).\n\nThis is referred to as windowing"],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Windowing</h2>\n\nIf we were using a static DataFrame to produce an aggregate count, we could use `groupBy()` and `count()`.\n\nInstead we accumulate counts within a sliding window, answering questions like \"How many records are we getting every second?\"\n\n**Sliding windows** \n\nThe windows overlap and a single event may be aggregated into multiple windows. \n\n**Tumbling Windows**\n\nThe windows do not overlap and a single event will be aggregated into only one window. \n\nThe diagram below shows sliding windows. \n\nThe following illustration, from the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a> guide, helps us understanding how it works:"],"metadata":{}},{"cell_type":"markdown","source":["<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-window.png\">"],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Event Time vs Receipt Time</h2>\n\n**Event Time** is the time at which the event occurred in the real world.\n\n**Event Time** is **NOT** something maintained by the Structured Streaming framework.\n\nAt best, Structured Streaming only knows about **Receipt Time** - the time a piece of data arrived in Spark."],"metadata":{}},{"cell_type":"markdown","source":["### What are some examples of **Event Time**? **of Receipt Time**?\n\n#### Examples of *Event Time*:\n* The timestamp recorded in each record of a log file\n* The instant at which an IoT device took a measurement\n* The moment a REST API received a request\n\n#### Examples of *Receipt Time*:\n* A timestamp added to a DataFrame the moment it was processed by Spark\n* The timestamp extracted from an hourly log file's file name\n* The time at which an IoT hub received a report of a device's measurement\n  - Presumably offset by some delay from when the measurement was taken"],"metadata":{}},{"cell_type":"markdown","source":["### What are some of the inherent problems with using **Receipt Time**?\n\nThe main problem with using **Receipt Time** is going to be with accuracy. For example:\n\n* The time between when an IoT device takes a measurement vs when it is reported can be off by several minutes.\n  - This could have significant ramifications to security and health devices, for example\n* The timestamp embedded in an hourly log file can be off by up to one hour making correlations to other events extremely difficult\n* The timestamp added by Spark as part of a DataFrame transformation can be off by hours to weeks to months depending on when the event occurred and when the job ran"],"metadata":{}},{"cell_type":"markdown","source":["### When might it be OK to use **Receipt Time** instead of **Event Time**?\n\nWhen accuracy is not a significant concern - that is **Receipt Time** is close enough to **Event Time**\n\nOne example would be for IoT events that can be delayed by minutes but the resolution of your query is by days or months (close enough)"],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Windowed Streaming Example</h2>\n\nFor this example, we will examine the files in `/mnt/training/sensor-data/accelerometer/time-series-stream.json/`.\n\nEach line in the file contains a JSON record with two fields: `time` and `action`\n\nNew files are being written to this directory continuously (aka streaming).\n\nTheoretically, there is no end to this process.\n\nLet's start by looking at the head of one such file:"],"metadata":{}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/file-0.json"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n\n### Define the Schema for the streaming content\n\nLet's try to analyze these files interactively.\n\nFirst configure a schema.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The schema must be specified for file-based Structured Streams.\nBecause of the simplicity of the schema, we can use the simpler, DDL-formatted, string representation of the schema."],"metadata":{}},{"cell_type":"code","source":["inputPath = \"dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/\"\n\njsonSchema = \"time timestamp, action string\""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Define a streaming Dataframe\n\nWith the schema defined, we can create the initial DataFrame `inputDf` and then `countsDF` which represents our aggregation:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import window, col\n\ninputDF = (spark\n  .readStream                                 # Returns an instance of DataStreamReader\n  .schema(jsonSchema)                         # Set the schema of the JSON data\n  .option(\"maxFilesPerTrigger\", 1)            # Treat a sequence of files as a stream, one file at a time\n  .json(inputPath)                            # Specifies the format, path and returns a DataFrame\n)\n\ncountsDF = (inputDF\n  .groupBy(col(\"action\"),                     # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))     # ...then by a 1 hour window\n  .count()                                    # For the aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"), # Elevate field to column\n          col(\"action\"),                      # Include count\n          col(\"count\"))                       # Include action\n  .orderBy(col(\"start\"), col(\"action\"))       # Sort by the start time and action\n)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### View Results\n\nTo view the results of our query, pass the DataFrame `countsDF` to the `display()` function."],"metadata":{}},{"cell_type":"code","source":["display(countsDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Performance Considerations\n\nIf you run that query, as is, it will take a surprisingly long time to start generating data. What's the cause of the delay?\n\nIf you expand the **Spark Jobs** component, you'll see something like this:\n\n<img src=\"https://files.training.databricks.com/images/structured-streaming-shuffle-partitions-200.png\"/>\n\nIt's our `groupBy()`. `groupBy()` causes a _shuffle_, and, by default, Spark SQL shuffles to 200 partitions. In addition, we're doing a _stateful_ aggregation: one that requires Structured Streaming to maintain and aggregate data over time.\n\nWhen doing a stateful aggregation, Structured Streaming must maintain an in-memory _state map_ for each window within each partition. For fault tolerance reasons, the state map has to be saved after a partition is processed, and it needs to be saved somewhere fault-tolerant. To meet those requirements, the Streaming API saves the maps to a distributed store. On some clusters, that will be HDFS. Databricks uses the DBFS.\n\nThat means that every time it finishes processing a window, the Streaming API writes its internal map to disk. The write has some overhead, typically between 1 and 2 seconds."],"metadata":{}},{"cell_type":"markdown","source":["### What's the cause of the delay?\n* `groupBy()` causes a **shuffle**\n* By default, this produces **200 partitions**\n* Plus a **stateful aggregation** to be maintained **over time**\n\nThis results in :\n* Maintenance of an **in-memory state map** for **each window** within **each partition**\n* Writing of the state map to a fault-tolerant store\n  * On some clusters, that will be HDFS\n  * Databricks uses the DBFS\n* Around 1 to 2 seconds overhead"],"metadata":{}},{"cell_type":"markdown","source":["### Shuffle Partition Best Practices\n\nOne way to reduce this overhead is to reduce the number of partitions Spark shuffles to.\n\nIn most cases, you want a 1-to-1 mapping of partitions to cores for streaming applications."],"metadata":{}},{"cell_type":"markdown","source":["### Run query with proper setting for shuffle partitions\n\nRerun the query below and notice the performance improvement.\n\nOnce the data is loaded, render a line graph with\n* **Keys** is set to `start`\n* **Series groupings** is set to `action`\n* **Values** is set to `count`"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\ndisplay(countsDF)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Stop all Streams\n\nWhen you are done, stop all the streaming jobs."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active: # Iterate over all active streams\n  s.stop()                     # Stop the stream"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Problem with Generating Many Windows</h2>\n\nWe are generating a window for every 1 hour aggregate.\n\n_Every window_ has to be separately persisted and maintained.\n\nOver time, this aggregated data will build up in the driver.\n\nThe end result being a massive slowdown if not an OOM Error.\n\n### How do we fix that problem?\n\nOne simple solution is to increase the size of our window (say, to 2 hours).\n\nThat way, we're generating fewer windows.\n\nBut if the job runs for a long time, we're still building up an unbounded set of windows.\n\nEventually, we could hit resource limits."],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Watermarking</h2>\n\nA better solution to the problem is to define a cut-off.\n\nA point after which Structured Streaming will commit windowed data to sink, or throw it away if the sink is console or memory as `display()` mimics.\n\nThat's what _watermarking_ allows us to do."],"metadata":{}},{"cell_type":"markdown","source":["### Refining our previous example\n\nBelow is our previous example with watermarking.\n\nWe're telling Structured Streaming to keep no more than 2 hours of aggregated data."],"metadata":{}},{"cell_type":"code","source":["watermarkedDF = (inputDF\n  .withWatermark(\"time\", \"2 hours\")           # Specify a 2-hour watermark\n  .groupBy(col(\"action\"),                     # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))     # ...then by a 1 hour window\n  .count()                                    # For each aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"), # Elevate field to column\n          col(\"action\"),                      # Include count\n          col(\"count\"))                       # Include action\n  .orderBy(col(\"start\"), col(\"action\"))       # Sort by the start time\n)\ndisplay(watermarkedDF)                        # Start the stream and display it"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Example Details\n\nIn the example above,\n* Data received 2 hour _past_ the watermark will be dropped.\n* Data received within 2 hours of the watermark will never be dropped.\n\nMore specifically, any data less than 2 hours behind the latest data processed till then is guaranteed to be aggregated.\n\nHowever, the guarantee is strict only in one direction.\n\nData delayed by more than 2 hours is not guaranteed to be dropped; it may or may not get aggregated.\n\nThe more delayed the data is, the less likely the engine is going to process it."],"metadata":{}},{"cell_type":"markdown","source":["### Stop all the streams"],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active: # Iterate over all active streams\n  s.stop()                     # Stop the stream\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Structured Streaming with Azure EventHubs]($./3.Streaming-With-Event-Hubs-Demo)"],"metadata":{}}],"metadata":{"name":"2.Time-Windows","notebookId":1947196994888219},"nbformat":4,"nbformat_minor":0}
