{"cells":[{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png\" style=\"float: left: margin: 20px\"/>\n\n# Structured Streaming Concepts\n\n## In this lesson you:\n* Stream data from a file and write it out to a distributed file system\n* List active streams\n* Stop active streams\n\n## Datasets Used\nData from\n`/mnt/training/definitive-guide/data/activity-data-stream.json/`\ncontains smartphone accelerometer samples from devices and users.\n\nThe file consists of the following columns:\n\n| Field          | Description |\n| ------------- | ----------- |\n| `Index` | unique identifier of event |\n| `x` | acceleration in x-dir |\n| `y` | acceleration in y-dir |\n| `z` | acceleration in z-dir |\n| `User` | unique user identifier |\n| `Model` | i.e Nexus4 |\n| `Device` | type of Model |\n| `gt` | transportation mode |\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The last column is named `gt`. This is an initialism for \"ground truth\". Wikipedia: <a href=\"https://en.wikipedia.org/wiki/Ground_truth\" target=\"_blank\">Ground Truth</a>"],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Getting Started</h2>\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Reading a Stream</h2>\n\nThe method `SparkSession.readStream` returns a `DataStreamReader` used to configure the stream.\n\nThere are a number of key points to the configuration of a `DataStreamReader`:\n* The schema\n* The type of stream: Files, Kafka, TCP/IP, etc\n* Configuration specific to the type of stream\n  * For files, the file type, the path to the files, max files, etc...\n  * For TCP/IP the server's address, port number, etc...\n  * For Kafka the server's address, port, topics, partitions, etc..."],"metadata":{}},{"cell_type":"markdown","source":["### The Schema\n\nEvery streaming DataFrame must have a schema - the definition of column names and data types.\n\nSome sources such as Pub/Sub sources like Kafka and Event Hubs define the schema for you.\n\nFor file-based streaming sources, the schema must be user-defined."],"metadata":{}},{"cell_type":"markdown","source":["### Why must a schema be specified for a streaming DataFrame?\n\nTo say that another way...\n\n### Why are streaming DataFrames unable to infer/read a schema?\n\nIf you have enough data, you can infer the schema.\n<br><br>\nIf you don't have enough data you run the risk of miss-inferring the schema.\n<br><br>\nFor example, you think you have all integers but the last value contains \"1.123\" (a float) or \"snoopy\" (a string).\n<br><br>\nWith a stream, we have to assume we don't have enough data because we are starting with zero records.\n<br><br>\nAnd unlike reading from a table or parquet file, there is nowhere from which to \"read\" the stream's schema.\n<br><br>\nFor this reason, we must specify the schema manually."],"metadata":{}},{"cell_type":"code","source":["# Here we define the schema using a DDL-formatted string (the SQL Data Definition Language).\ndataSchema = \"Recorded_At timestamp, Device string, Index long, Model string, User string, _corrupt_record String, gt string, x double, y double, z double\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Configuring a File Stream\n\nIn our example below, we will be consuming files written continuously to a pre-defined directory.\n\nTo control how much data is pulled into Spark at once, we can specify the option `maxFilesPerTrigger`.\n\nIn our example below, we will be reading in only one file for every trigger interval:\n\n`.option(\"maxFilesPerTrigger\", 1)`\n\nBoth the location and file type are specified with the following call, which itself returns a `DataFrame`:\n\n`.json(dataPath)`"],"metadata":{}},{"cell_type":"code","source":["dataPath = \"dbfs:/mnt/training/definitive-guide/data/activity-data-stream.json\"\ninitialDF = (spark\n  .readStream                            # Returns DataStreamReader\n  .option(\"maxFilesPerTrigger\", 1)       # Force processing of only 1 file per trigger\n  .schema(dataSchema)                    # Required for all streaming DataFrames\n  .json(dataPath)                        # The stream's source directory and file type\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["And with the initial `DataFrame`, we can apply some transformations:"],"metadata":{}},{"cell_type":"code","source":["streamingDF = (initialDF\n  .withColumnRenamed(\"Index\", \"User_ID\")  # Pick a \"better\" column name\n  .drop(\"_corrupt_record\")                # Remove an unnecessary column\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Streaming DataFrames\n\nOther than the call to `spark.readStream`, it looks just like any other `DataFrame`.\n\nBut is it a \"streaming\" `DataFrame`?\n\nYou can differentiate between a \"static\" and \"streaming\" `DataFrame` with the following call:"],"metadata":{}},{"cell_type":"code","source":["# Static vs Streaming?\nstreamingDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Unsupported Operations\n\nMost operations on a \"streaming\" DataFrame are identical to a \"static\" DataFrame.\n\nThere are some exceptions to this.\n\nOne such example would be to sort our never-ending stream by `Recorded_At`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ntry:\n  sortedDF = streamingDF.orderBy(col(\"Recorded_At\").desc())\n  display(sortedDF)\nexcept:\n  print(\"Sorting is not supported on an unaggregated stream\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\nSorting is one of a handful of operations that is either too complex or logically not possible to do with a stream.\n\nFor more information on this topic, see the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\" target=\"_blank\">Structured Streaming Programming Guide / Unsupported Operations</a>.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We will see in the following module how we can sort an **aggregated** stream."],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Writing a Stream</h2>\n\nThe method `DataFrame.writeStream` returns a `DataStreamWriter` used to configure the output of the stream.\n\nThere are a number of parameters to the `DataStreamWriter` configuration:\n* Query's name (optional) - This name must be unique among all the currently active queries in the associated SQLContext.\n* Trigger (optional) - Default value is `ProcessingTime(0`) and it will run the query as fast as possible.\n* Checkpointing directory (optional for pup/sub sinks)\n* Output mode\n* Output sink\n* Configuration specific to the output sink, such as:\n  * The host, port and topic of the receiving Kafka server\n  * The file format and final destination of files\n  * A <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=foreach#pyspark.sql.streaming.DataStreamWriter.foreach\"target=\"_blank\">custom sink</a> via `writeStream.foreach(...)`\n\nOnce the configuration is completed, we can trigger the job with a call to `.start()`"],"metadata":{}},{"cell_type":"markdown","source":["### Triggers\n\nThe trigger specifies when the system should process the next set of data.\n\n| Trigger Type                           | Example | Notes |\n|----------------------------------------|-----------|-------------|\n| Unspecified                            |  | _DEFAULT_- The query will be executed as soon as the system has completed processing the previous query |\n| Fixed interval micro-batches           | `.trigger(Trigger.ProcessingTime(\"6 hours\"))` | The query will be executed in micro-batches and kicked off at the user-specified intervals |\n| One-time micro-batch                   | `.trigger(Trigger.Once())` | The query will execute _only one_ micro-batch to process all the available data and then stop on its own |\n| Continuous w/fixed checkpoint interval | `.trigger(Trigger.Continuous(\"1 second\"))` | The query will be executed in a low-latency, <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing\" target = \"_blank\">continuous processing mode</a>. _EXPERIMENTAL_ in 2.3.2 |\n\nIn the example below, you will be using a fixed interval of 3 seconds:\n\n`.trigger(Trigger.ProcessingTime(\"3 seconds\"))`"],"metadata":{}},{"cell_type":"markdown","source":["### Checkpointing\n\nA <b>checkpoint</b> stores the current state of your streaming job to a reliable storage system such as Azure Blob Storage or HDFS. It does not store the state of your streaming job to the local file system of any node in your cluster.\n\nTogether with write ahead logs, a terminated stream can be restarted and it will continue from where it left off.\n\nTo enable this feature, you only need to specify the location of a checkpoint directory:\n\n`.option(\"checkpointLocation\", checkpointPath)`"],"metadata":{}},{"cell_type":"markdown","source":["Points to consider:\n* If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n* For some sinks, you will get an error if you do not specify a checkpoint directory:<br/>\n`analysisException: 'checkpointLocation must be specified either through option(\"checkpointLocation\", ...)..`\n* Also note that every streaming job should have its own checkpoint directory: no sharing."],"metadata":{}},{"cell_type":"markdown","source":["### Output Modes\n\n| Mode   | Example | Notes |\n| ------------- | ----------- |\n| **Complete** | `.outputMode(\"complete\")` | The entire updated Result Table is written to the sink. The individual sink implementation decides how to handle writing the entire table. |\n| **Append** | `.outputMode(\"append\")`     | Only the new rows appended to the Result Table since the last trigger are written to the sink. |\n| **Update** | `.outputMode(\"update\")`     | Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. Since Spark 2.1.1 |\n\nIn the example below, we are writing to a Parquet directory which only supports the `append` mode:\n\n`dsw.outputMode(\"append\")`"],"metadata":{}},{"cell_type":"markdown","source":["### Output Sinks\n\n`DataStreamWriter.format` accepts the following values, among others:\n\n| Output Sink | Example                                          | Notes |\n| ----------- | ------------------------------------------------ | ----- |\n| **File**    | `dsw.format(\"parquet\")`, `dsw.format(\"csv\")`...  | Dumps the Result Table to a file. Supports Parquet, json, csv, etc.|\n| **Kafka**   | `dsw.format(\"kafka\")`      | Writes the output to one or more topics in Kafka |\n| **Console** | `dsw.format(\"console\")`    | Prints data to the console (useful for debugging) |\n| **Memory**  | `dsw.format(\"memory\")`     | Updates an in-memory table, which can be queried through Spark SQL or the DataFrame API |\n| **foreach** | `dsw.foreach(writer: ForeachWriter)` | This is your \"escape hatch\", allowing you to write your own type of sink. |\n| **Delta**    | `dsw.format(\"delta\")`     | A proprietary sink |\n\nIn the example below, we will be appending files to a Parquet directory and specifying its location with this call:\n\n`.format(\"parquet\").start(outputPathDir)`"],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Let's Do Some Streaming</h2>\n\nIn the cell below, we write data from a streaming query to `outputPathDir`.\n\nThere are a couple of things to note below:\n0. We are giving the query a name via the call to `.queryName` \n0. Spark begins running jobs once we call `.start`\n0. The call to `.start` returns a `StreamingQuery` object"],"metadata":{}},{"cell_type":"code","source":["basePath = userhome + \"/structured-streaming-concepts/python\" # A working directory for our streaming app\ndbutils.fs.mkdirs(basePath)                                   # Make sure that our working directory exists\noutputPathDir = basePath + \"/output.parquet\"                  # A subdirectory for our output\ncheckpointPath = basePath + \"/checkpoint\"                     # A subdirectory for our checkpoint & W-A logs\n\nstreamingQuery = (streamingDF                                 # Start with our \"streaming\" DataFrame\n  .writeStream                                                # Get the DataStreamWriter\n  .queryName(\"stream_1p\")                                     # Name the query\n  .trigger(processingTime=\"3 seconds\")                        # Configure for a 3-second micro-batch\n  .format(\"parquet\")                                          # Specify the sink type, a Parquet file\n  .option(\"checkpointLocation\", checkpointPath)               # Specify the location of checkpoint files & W-A logs\n  .outputMode(\"append\")                                       # Write only new data to the \"file\"\n  .start(outputPathDir)                                       # Start the job, writing to the specified directory\n)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Managing Streaming Queries</h2>\n\nWhen a query is started, the `StreamingQuery` object can be used to monitor and manage the query.\n\n| Method    |  Description |\n| ----------- | ------------------------------- |\n|`id`| get unique identifier of the running query that persists across restarts from checkpoint data |\n|`runId`| get unique id of this run of the query, which will be generated at every start/restart |\n|`name`| get name of the auto-generated or user-specified name |\n|`explain()`| print detailed explanations of the query |\n|`stop()`| stop query |\n|`awaitTermination()`| block until query is terminated, with stop() or with error |\n|`exception`| exception if query terminated with error |\n|`recentProgress`| array of most recent progress updates for this query |\n|`lastProgress`| most recent progress update of this streaming query |"],"metadata":{}},{"cell_type":"code","source":["streamingQuery.recentProgress"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Additionally, we can iterate over a list of active streams:"],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:         # Iterate over all streams\n  print(\"{}: {}\".format(s.id, s.name)) # Print the stream's id and name"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["The code below stops the `streamingQuery` defined above and introduces `awaitTermination()`\n\n`awaitTermination()` will block the current thread\n* Until the stream stops or\n* Until the specified timeout elapses"],"metadata":{}},{"cell_type":"code","source":["streamingQuery.awaitTermination(5)      # Stream for another 5 seconds while the current thread blocks\nstreamingQuery.stop()                   # Stop the stream"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["-sandbox\n\n<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> The Display function</h2>\n\nWithin the Databricks notebooks, we can use the `display()` function to render a live plot\n\nWhen you pass a \"streaming\" `DataFrame` to `display()`:\n* A \"memory\" sink is being used\n* The output mode is complete\n* The query name is specified with the `streamName` parameter\n* The trigger is specified with the `trigger` parameter\n* The checkpointing location is specified with the `checkpointLocation`\n\n`display(myDF, streamName = \"myQuery\")`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We just programmatically stopped our only streaming query in the previous cell. In the cell below, `display` will automatically start our streaming DataFrame, `streamingDF`.  We are passing `stream_2p` as the name for this newly started stream."],"metadata":{}},{"cell_type":"code","source":["myStream = \"stream_2p\"\ndisplay(streamingDF, streamName = myStream)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Using the value passed to `streamName` in the call to `display`, we can programatically access this specific stream:"],"metadata":{}},{"cell_type":"code","source":["print(\"Looking for {}\".format(myStream))\n\nfor stream in spark.streams.active:      # Loop over all active streams\n  if stream.name == myStream:            # Single out \"streamWithTimestamp\"\n    print(\"Found {} ({})\".format(stream.name, stream.id))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Since the `streamName` get's registered as a temporary table pointing to the memory sink, we can use SQL to query the sink."],"metadata":{}},{"cell_type":"code","source":["spark.catalog.listTables()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Stop all remaining streams."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Clean up our directories"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(basePath, True)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## End-to-end Fault Tolerance\n\nStructured Streaming ensures end-to-end exactly-once fault-tolerance guarantees through _checkpointing_ and <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\">Write Ahead Logs</a>.\n\nStructured Streaming sources, sinks, and the underlying execution engine work together to track the progress of stream processing. If a failure occurs, the streaming engine attempts to restart and/or reprocess the data.\nFor best practices on recovering from a failed streaming query see <a href=\"\">docs</a>.\n\nThis approach _only_ works if the streaming source is replayable. To ensure fault-tolerance, Structured Streaming assumes that every streaming source has offsets, akin to:\n\n* <a target=\"_blank\" href=\"https://kafka.apache.org/documentation/#intro_topics\">Kafka message offsets</a>\n* <a href=\"https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features\" target=\"_blank\"> Event Hubs offsets</a>\n\n\nAt a high level, the underlying streaming mechanism relies on a couple approaches:\n\n* First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n* Next, the streaming sinks are designed to be _idempotent_â€”that is, multiple writes of the same data (as identified by the offset) do _not_ result in duplicates being written to the sink.\n\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure **end-to-end, exactly-once semantics** under any failure condition."],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Summary</h2>\n\nWe use `readStream` to read streaming input from a variety of input sources and create a DataFrame.\n\nNothing happens until we invoke `writeStream` or `display`.\n\nUsing `writeStream` we can write to a variety of output sinks. Using `display` we draw LIVE bar graphs, charts and other plot types in the notebook."],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Review Questions</h2>\n\n**Q:** What do `readStream` and `writeStream` do?<br>\n**A:** `readStream` creates a streaming DataFrame.<br>`writeStream` sends streaming data to a directory or other type of output sink.\n\n**Q:** What does `display` output if it is applied to a DataFrame created via `readStream`?<br>\n**A:** `display` sends streaming data to a LIVE graph!\n\n**Q:** When you do a write stream command, what does this option do `outputMode(\"append\")` ?<br>\n**A:** This option takes on the following values and their respective meanings:\n* <b>append</b>: add only new records to output sink\n* <b>complete</b>: rewrite full output - applicable to aggregations operations\n* <b>update</b>: update changed records in place\n\n**Q:** What happens if you do not specify `option(\"checkpointLocation\", pointer-to-checkpoint directory)`?<br>\n**A:** When the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n**Q:** How do you view the list of active streams?<br>\n**A:** Invoke `spark.streams.active`.\n\n**Q:** How do you verify whether `streamingQuery` is running (boolean output)?<br>\n**A:** Invoke `spark.streams.get(streamingQuery.id).isActive`."],"metadata":{}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Additional Topics &amp; Resources</h2>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tathagata Das. This is an excellent video describing how Structured Streaming works.\n* <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html#id2\" target=\"_blank\">Failed Streaming Query Recovery</a> Best Practices for Recovery.\n* <a href=\"https://databricks.com/blog/2018/03/20/low-latency-continuous-processing-mode-in-structured-streaming-in-apache-spark-2-3-0.html\" target=\"_blank\">Continuous Processing Mode</a> Lowest possible latency stream processing.  Currently Experimental."],"metadata":{}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Working with Time Windows]($./2.Time-Windows)"],"metadata":{}}],"metadata":{"name":"1.Structured-Streaming-Concepts","notebookId":1947196994888137},"nbformat":4,"nbformat_minor":0}
