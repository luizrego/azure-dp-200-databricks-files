{"cells":[{"cell_type":"markdown","source":["# Structured Streaming with Azure EventHubs \n\n## Datasets Used\nThis notebook will consumn data being published through an EventHub with the following schema:\n\n- `Index`\n- `Arrival_Time`\n- `Creation_Time`\n- `x`\n- `y`\n- `z`\n- `User`\n- `Model`\n- `Device`\n- `gt`\n- `id`\n- `geolocation`\n\n## Library Requirements\n\n1. the Maven library with coordinate `com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.7`\n   - this allows Databricks `spark` session to communicate with an Event Hub\n2. the Python library `azure-eventhub`\n   - this is allows the Python kernel to stream content to an Event Hub\n\nThe next cell walks you through installing the Maven library. A couple cells below that, we automatically install the Python library using `dbutils.library.installPyPI`."],"metadata":{}},{"cell_type":"markdown","source":["## Lab Setup\n\nIf you are running in an Azure Databricks environment that is already pre-configured with the libraries you need, you can skip to the next cell. To use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Clusters**, then select your cluster in the list. If it's not running, start it now.\n\n  ![Select cluster](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/select-cluster.png)\n\n2. Select the **Libraries** tab (1), then select **Install New** (2). In the Install Library dialog, select **Maven** under Library Source (3). Under Coordinates, paste **com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.7** (4), then select **Install**.\n  \n  ![Databricks new Maven library](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/install-eventhubs-spark-library.png)\n\n3. Wait until the library successfully installs before continuing.\n\n  ![Library installed](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/eventhubs-spark-library-installed.png)\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following two cells to install the `azure-eventhub` Python library and configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["# This library allows the Python kernel to stream content to an Event Hub:\ndbutils.library.installPyPI('azure-eventhub')\n\n#best practice is to restart python after installing libraries\ndbutils.library.restartPython() "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["The following cell sets up a local streaming file read that we'll be writing to Event Hubs."],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Streaming-Demo-Setup"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["In order to reach Event Hubs, you will need to insert the connection string-primary key you acquired at the end of the Getting Started notebook in this module. You acquired this from the Azure Portal, and copied it into Notepad.exe or another text editor.\n\n> Read this article to learn [how to acquire the connection string for an Event Hub](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create) in your own Azure Subscription."],"metadata":{}},{"cell_type":"code","source":["event_hub_connection_string = \"Endpoint=sb://databricksdemoeventhubs3.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=GuAj2zdcehfXNJXjeeXB6eEOWR4xaNcGwra0LaG9N+Y=\" #{your-event-hubs-connection-string-primary-key}"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Azure Event Hubs</h2>\n\nMicrosoft Azure Event Hubs is a fully managed, real-time data ingestion service.\nYou can stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\nIt integrates seamlessly with a host of other Azure services.\n\nEvent Hubs can be used in a variety of applications such as\n* Anomaly detection (fraud/outliers)\n* Application logging\n* Analytics pipelines, such as clickstreams\n* Archiving data\n* Transaction processing\n* User telemetry processing\n* Device telemetry streaming\n* <b>Live dashboarding</b>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n### Define Connection Strings\n\nThis cell defines the necessary connection strings.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> This is **demo-only**."],"metadata":{}},{"cell_type":"code","source":["%python\n\nevent_hub_name = \"databricks-demo-eventhub\"\nconnection_string = event_hub_connection_string + \";EntityPath=\" + event_hub_name\n\nprint(\"Consumer Connection String: {}\".format(connection_string))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Write Stream to Event Hub to Produce Stream"],"metadata":{}},{"cell_type":"code","source":["%python\n\nehWriteConf = {\n  'eventhubs.connectionString' : connection_string\n}\n\ncheckpointPath = userhome + \"/event-hub/write-checkpoint\"\ndbutils.fs.rm(checkpointPath,True)\n\n(activityStreamDF\n  .writeStream\n  .format(\"eventhubs\")\n  .options(**ehWriteConf)\n  .option(\"checkpointLocation\", checkpointPath)\n  .start())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Event Hubs Configuration</h2>\n\nAssemble the following:\n* A `startingEventPosition` as a JSON string\n* An `EventHubsConf`\n  * to include a string with connection credentials\n  * to set a starting position for the stream read\n  * to throttle Event Hubs' processing of the streams"],"metadata":{}},{"cell_type":"code","source":["%python\n\nimport json\n\n# Create the starting position Dictionary\nstartingEventPosition = {\n  \"offset\": \"-1\",\n  \"seqNo\": -1,            # not in use\n  \"enqueuedTime\": None,   # not in use\n  \"isInclusive\": True\n}\n\neventHubsConf = {\n  \"eventhubs.connectionString\" : connection_string,\n  \"eventhubs.startingPosition\" : json.dumps(startingEventPosition),\n  \"setMaxEventsPerTrigger\": 100\n}"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### READ Stream using EventHub\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`."],"metadata":{}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import col\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\neventStreamDF = (spark.readStream\n  .format(\"eventhubs\")\n  .options(**eventHubsConf)\n  .load()\n)\n\neventStreamDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Most of the fields in this response are metadata describing the state of the Event Hubs stream. We are specifically interested in the `body` field, which contains our JSON payload.\n\nNoting that it's encoded as binary, as we select it, we'll cast it to a string."],"metadata":{}},{"cell_type":"code","source":["%python\nbodyDF = eventStreamDF.select(col(\"body\").cast(\"STRING\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Each line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\nNotice that nothing happens until you engage an action, i.e. a `display()` or `writeStream`."],"metadata":{}},{"cell_type":"code","source":["%python\ndisplay(bodyDF, streamName= \"bodyDF\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["While we can see our JSON data now that it's cast to string type, we can't directly manipulate it.\n\nBefore proceeding, stop this stream. We'll continue building up transformations against this streaming DataFrame, and a new action will trigger an additional stream."],"metadata":{}},{"cell_type":"code","source":["%python\nfor s in spark.streams.active:\n  if s.name == \"bodyDF\":\n    s.stop()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["-sandbox\n## <img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Parse the JSON payload\n\nThe EventHub acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data in the JSON format.\n\nIf desired, we could save this as raw bytes or strings and parse these records further downstream in our processing.\n\nHere, we'll directly parse our data so we can interact with the fields.\n\nThe first step is to define the schema for the JSON payload.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Both time fields are encoded as `LongType` here because of non-standard formatting."],"metadata":{}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType, DoubleType\n\nschema = StructType([\n  StructField(\"Arrival_Time\", LongType(), True),\n  StructField(\"Creation_Time\", LongType(), True),\n  StructField(\"Device\", StringType(), True),\n  StructField(\"Index\", LongType(), True),\n  StructField(\"Model\", StringType(), True),\n  StructField(\"User\", StringType(), True),\n  StructField(\"gt\", StringType(), True),\n  StructField(\"x\", DoubleType(), True),\n  StructField(\"y\", DoubleType(), True),\n  StructField(\"z\", DoubleType(), True),\n  StructField(\"geolocation\", StructType([\n    StructField(\"PostalCode\", StringType(), True),\n    StructField(\"StateProvince\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True)\n  ]), True),\n  StructField(\"id\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Parse the data\n\nNext we can use the function `from_json` to parse out the full message with the schema specified above.\n\nWhen parsing a value from JSON, we end up with a single column containing a complex object."],"metadata":{}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import col, from_json\n\nparsedEventsDF = bodyDF.select(\n  from_json(col(\"body\"), schema).alias(\"json\"))\n\nparsedEventsDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Note that we can further parse this to flatten the schema entirely and properly cast our time fields."],"metadata":{}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import from_unixtime\n\nflatSchemaDF = (parsedEventsDF\n  .select(from_unixtime(col(\"json.Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"json.Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"json.Device\").alias(\"Device\"),\n          col(\"json.Index\").alias(\"Index\"),\n          col(\"json.Model\").alias(\"Model\"),\n          col(\"json.User\").alias(\"User\"),\n          col(\"json.gt\").alias(\"gt\"),\n          col(\"json.x\").alias(\"x\"),\n          col(\"json.y\").alias(\"y\"),\n          col(\"json.z\").alias(\"z\"),\n          col(\"json.id\").alias(\"id\"),\n          col(\"json.geolocation.country\").alias(\"country\"),\n          col(\"json.geolocation.city\").alias(\"city\"),\n          col(\"json.geolocation.PostalCode\").alias(\"PostalCode\"),\n          col(\"json.geolocation.StateProvince\").alias(\"StateProvince\"))\n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["This flat schema provides us the ability to view each nested field as a column."],"metadata":{}},{"cell_type":"code","source":["%python\ndisplay(flatSchemaDF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Stop all active streams"],"metadata":{}},{"cell_type":"code","source":["%python\nfor s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["#### Event Hubs FAQ\n\nThis [FAQ](https://github.com/Azure/azure-event-hubs-spark/blob/master/FAQ.md) can be an invaluable reference for occasional Spark-EventHub debugging."],"metadata":{}}],"metadata":{"name":"3.Streaming-With-Event-Hubs-Demo","notebookId":1947196994888183},"nbformat":4,"nbformat_minor":0}
