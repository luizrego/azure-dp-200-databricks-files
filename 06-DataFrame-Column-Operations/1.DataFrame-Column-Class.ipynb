{"cells":[{"cell_type":"markdown","source":["#DataFrame Column Class\n\n** Data Source **\n* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n* Size on Disk: ~23 MB\n* Type: Compressed Parquet File\n* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n\n**Technical Accomplishments:**\n* Continue exploring the `DataFrame` set of APIs.\n* Introduce the `Column` class"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n\nWe will be using the same data source as our previous notebook.\n\nAs such, we can go ahead and start by creating our initial `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\nspark.conf.set(sasEntity, sasToken)\n\nparquetFile = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n  .read                     # Our DataFrameReader\n  .parquet(parquetFile)     # Returns an instance of DataFrame\n  .cache()                  # cache the data\n)\nprint(pagecountsEnAllDF)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Let's take another look at the number of records in our `DataFrame`"],"metadata":{}},{"cell_type":"code","source":["total = pagecountsEnAllDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Now let's take another peek at our data..."],"metadata":{}},{"cell_type":"code","source":["display(pagecountsEnAllDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["As we view the data, we can see that there is no real rhyme or reason as to how the data is sorted.\n* We cannot even tell if the column **project** is sorted - we are seeing only the first 1,000 of some 2.3 million records.\n* The column **article** is not sorted as evident by the article **A_Little_Boy_Lost** appearing between a bunch of articles starting with numbers and symbols.\n* The column **requests** is clearly not sorted.\n* And our **bytes_served** contains nothing but zeros.\n\nSo let's start by sorting our data. In doing this, we can answer the following question:\n\nWhat are the top 10 most requested articles?"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) orderBy(..) & sort(..)\n\nIf you look at the API docs, `orderBy(..)` is described like this:\n> Returns a new Dataset sorted by the given expressions.\n\nBoth `orderBy(..)` and `sort(..)` arrange all the records in the `DataFrame` as specified.\n* Like `distinct()` and `dropDuplicates()`, `sort(..)` and `orderBy(..)` are aliases for each other.\n  * `sort(..)` appealing to functional programmers.\n  * `orderBy(..)` appealing to developers with an SQL background.\n* Like `orderBy(..)` there are two variants of these two methods:\n  * `orderBy(Column)`\n  * `orderBy(String)`\n  * `sort(Column)`\n  * `sort(String)`\n\nAll we need to do now is sort our previous `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["sortedDF = (pagecountsEnAllDF\n  .orderBy(\"requests\")\n)\nsortedDF.show(10, False)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["As you can see, we are not sorting correctly.\n\nWe need to reverse the sort.\n\nOne might conclude that we could make a call like this:\n\n`pagecountsEnAllDF.orderBy(\"requests desc\")`\n\nTry it in the cell below:"],"metadata":{}},{"cell_type":"code","source":["# Uncomment and try this:\n# pagecountsEnAllDF.orderBy(\"requests desc\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Why does this not work?\n* The `DataFrames` API is built upon an SQL engine.\n* There is a lot of familiarity with this API and SQL syntax in general.\n* The problem is that `orderBy(..)` expects the name of the column.\n* What we specified was an SQL expression in the form of **requests desc**.\n* What we need is a way to programmatically express such an expression.\n* This leads us to the second variant, `orderBy(Column)` and more specifically, the class `Column`.\n\n** *Note:* ** *Some of the calls in the `DataFrames` API actually accept SQL expressions.*<br/>\n*While these functions will appear in the docs as `someFunc(String)` it's very*<br>\n*important to thoroughly read and understand what the parameter actually represents.*"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Column Class\n\nThe `Column` class is an object that encompasses more than just the name of the column, but also column-level-transformations, such as sorting in a descending order.\n\nThe first question to ask is how do I create a `Column` object?\n\nIn Scala we have these options:"],"metadata":{}},{"cell_type":"markdown","source":["** *Note:* ** *We are showing both the Scala and Python versions below for comparison.*<br/>\n*Make sure to run only the one cell for your notebook's default language (Scala or Python)*"],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Scala & Python both support accessing a column from a known DataFrame\n// Uncomment this if you are using the Scala version of this notebook\n// val columnA = pagecountsEnAllDF(\"requests\")    \n\n// This option is Scala specific, but is arugably the cleanest and easy to read.\nval columnB = $\"requests\"          \n\n// If we import ...sql.functions, we get a couple of more options:\nimport org.apache.spark.sql.functions._\n\n// This uses the col(..) function\nval columnC = col(\"requests\")\n\n// This uses the expr(..) function which parses an SQL Expression\nval columnD = expr(\"a + 1\")\n\n// This uses the lit(..) to create a literal (constant) value.\nval columnE = lit(\"abc\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["In Python we have these options:"],"metadata":{}},{"cell_type":"code","source":["%python\n\n# Scala & Python both support accessing a column from a known DataFrame\n# Uncomment this if you are using the Python version of this notebook\n# columnA = pagecountsEnAllDF[\"requests\"]\n\n# The $\"column-name\" version that works for Scala does not work in Python\n# columnB = $\"requests\"      \n\n# If we import ...sql.functions, we get a couple of more options:\nfrom pyspark.sql.functions import *\n\n# This uses the col(..) function\ncolumnC = col(\"requests\")\n\n# This uses the expr(..) function which parses an SQL Expression\ncolumnD = expr(\"a + 1\")\n\n# This uses the lit(..) to create a literal (constant) value.\ncolumnE = lit(\"abc\")\n\n# Print the type of each attribute\nprint(\"columnC: {}\".format(columnC))\nprint(\"columnD: {}\".format(columnD))\nprint(\"columnE: {}\".format(columnE))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["In the case of Scala, the cleanest version is the **$\"column-name\"** variant.\n\nIn the case of Python, the cleanest version is the **col(\"column-name\")** variant.\n\nSo with that, we can now create a `Column` object, and apply the `desc()` operation to it:\n\n** *Note:* ** *We are introducing `...sql.functions` specifically for creating `Column` objects.*<br/>\n*We will be reviewing the multitude of other commands available from this part of the API in future notebooks.*"],"metadata":{}},{"cell_type":"code","source":["column = col(\"requests\").desc()\n\n# Print the column type\nprint(\"column:\", column)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["And now we can piece it all together..."],"metadata":{}},{"cell_type":"code","source":["sortedDescDF = (pagecountsEnAllDF\n  .orderBy( col(\"requests\").desc() )\n)  \nsortedDescDF.show(10, False) # The top 10 is good enough for now"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["It should be of no surprise that the **Main_Page** (in both the Wikipedia and Wikimedia projects) is the most requested page.\n\nFollowed shortly after that is **Special:Search**, Wikipedia's search page.\n\nAnd if you consider that this data was captured in the August before the 2016 presidential election, the Trumps will be one of the most requested pages on Wikipedia."],"metadata":{}},{"cell_type":"markdown","source":["### Review Column Class\n\nThe `Column` objects provide us a programmatic way to build up SQL-ish expressions.\n\nBesides the `Column.desc()` operation we used above, we have a number of other operations that can be performed on a `Column` object.\n\nHere is a preview of the various functions - we will cover many of these as we progress through the class:\n\n**Column Functions**\n* Various mathematical functions such as add, subtract, multiply & divide\n* Various bitwise operators such as AND, OR & XOR\n* Various null tests such as `isNull()`, `isNotNull()` & `isNaN()`.\n* `as(..)`, `alias(..)` & `name(..)` - Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).\n* `between(..)` - A boolean expression that is evaluated to true if the value of this expression is between the given columns.\n* `cast(..)` & `astype(..)` - Convert the column into type dataType.\n* `asc(..)` - Returns a sort expression based on the ascending order of the given column name.\n* `desc(..)` - Returns a sort expression based on the descending order of the given column name.\n* `startswith(..)` - String starts with.\n* `endswith(..)` - String ends with another string literal.\n* `isin(..)` - A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\n* `like(..)` - SQL like expression\n* `rlike(..)` - SQL RLIKE expression (LIKE with Regex).\n* `substr(..)` - An expression that returns a substring.\n* `when(..)` & `otherwise(..)` - Evaluates a list of conditions and returns one of multiple possible result expressions.\n\nThe complete list of functions differs from language to language."],"metadata":{}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Work with Column expressions]($./2.DataFrame-Column-Expressions)"],"metadata":{}}],"metadata":{"name":"1.DataFrame-Column-Class","notebookId":1947196994889027},"nbformat":4,"nbformat_minor":0}
