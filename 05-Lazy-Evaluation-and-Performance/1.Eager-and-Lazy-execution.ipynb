{"cells":[{"cell_type":"markdown","source":["# Describe the difference between eager and lazy execution"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Laziness By Design\n\nFundamental to Apache Spark are the notions that\n* Transformations are **LAZY**\n* Actions are **EAGER**\n\nThe following code condenses the logic from the DataFrames modules in this learning path, and uses the DataFrames API to:\n- Specify a schema, format, and file source for the data to be loaded\n- Select columns to `GROUP BY`\n- Aggregate with a `COUNT`\n- Provide an alias name for the aggregate output\n- Specify a column to sort on\n\nThis cell defines a series of **transformations**. By definition, this logic will result in a DataFrame and will not trigger any jobs."],"metadata":{}},{"cell_type":"code","source":["schemaDDL = \"NAME STRING, STATION STRING, LATITUDE FLOAT, LONGITUDE FLOAT, ELEVATION FLOAT, DATE DATE, UNIT STRING, TAVG FLOAT\"\n\nsourcePath = \"/mnt/training/weather/StationData/stationData.parquet/\"\n\ncountsDF = (spark.read\n  .format(\"parquet\")\n  .schema(schemaDDL)\n  .load(sourcePath)\n  .groupBy(\"NAME\", \"UNIT\").count()\n  .withColumnRenamed(\"count\", \"counts\")\n  .orderBy(\"NAME\")\n)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Because `display` is an **action**, a job _will_ be triggered, as logic is executed against the specified data to return a result."],"metadata":{}},{"cell_type":"code","source":["display(countsDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Why is Laziness So Important?\n\nLaziness is at the core of Scala and Spark.\n\nIt has a number of benefits:\n* Not forced to load all data at step #1\n  * Technically impossible with **REALLY** large datasets.\n* Easier to parallelize operations\n  * N different transformations can be processed on a single data element, on a single thread, on a single machine.\n* Optimizations can be applied prior to code compilation"],"metadata":{}}],"metadata":{"name":"1.Eager-and-Lazy-execution","notebookId":1947196994889173},"nbformat":4,"nbformat_minor":0}
