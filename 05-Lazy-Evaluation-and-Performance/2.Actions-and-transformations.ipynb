{"cells":[{"cell_type":"markdown","source":["# Actions & Transformations"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["schemaDDL = \"NAME STRING, STATION STRING, LATITUDE FLOAT, LONGITUDE FLOAT, ELEVATION FLOAT, DATE DATE, UNIT STRING, TAVG FLOAT\"\n\nsourcePath = \"/mnt/training/weather/StationData/stationData.parquet/\"\n\ncountsDF = (spark.read\n  .format(\"parquet\")\n  .schema(schemaDDL)\n  .load(sourcePath)\n  .groupBy(\"NAME\", \"UNIT\").count()\n  .withColumnRenamed(\"count\", \"counts\")\n  .orderBy(\"NAME\")\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["-sandbox\n##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Actions\n\nIn production code, actions will generally **write data to persistent storage** using the DataFrameWriter discussed in other Azure Databricks learning path modules.\n\nDuring interactive code development in Databricks notebooks, the `display` method will frequently be used to **materialize a view of the data** after logic has been applied.\n\nA number of other actions provide the ability to return previews or specify physical execution plans for how logic will map to data. For the complete list, review the [API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).\n\n| Method | Return | Description |\n|--------|--------|-------------|\n| `collect()` | Collection | Returns an array that contains all of Rows in this Dataset. |\n| `count()` | Long | Returns the number of rows in the Dataset. |\n| `first()` | Row | Returns the first row. |\n| `foreach(f)` | - | Applies a function f to all rows. |\n| `foreachPartition(f)` | - | Applies a function f to each partition of this Dataset. |\n| `head()` | Row | Returns the first row. |\n| `reduce(f)` | Row | Reduces the elements of this Dataset using the specified binary function. |\n| `show(..)` | - | Displays the top 20 rows of Dataset in a tabular form. |\n| `take(n)` | Collection | Returns the first n rows in the Dataset. |\n| `toLocalIterator()` | Iterator | Return an iterator that contains all of Rows in this Dataset. |\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Actions such as `collect` can lead to out of memory errors by forcing the collection of all data."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Transformations\n\nTransformations have the following key characteristics:\n* They eventually return another `DataFrame`.\n* They are immutable - that is each instance of a `DataFrame` cannot be altered once it's instantiated.\n  * This means other optimizations are possible - such as the use of shuffle files (to be discussed in detail later)\n* Are classified as either a Wide or Narrow operation\n\nMost operations in Spark are **transformations**. While many transformations are [DataFrame operations](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), writing efficient Spark code will require importing methods from the `sql.functions` module, which contains [transformations corresponding to SQL built-in operations](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)."],"metadata":{}},{"cell_type":"markdown","source":["## Types of Transformations\n\nA transformation may be wide or narrow.\n\nA wide transformation requires sharing data across workers. \n\nA narrow transformation can be applied per partition/worker with no need to share or shuffle data to other workers."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Narrow Transformations\n\nThe data required to compute the records in a single partition reside in at most one partition of the parent Dataframe.\n\nExamples include:\n* `filter(..)`\n* `drop(..)`\n* `coalesce()`\n\n![](https://databricks.com/wp-content/uploads/2018/05/Narrow-Transformation.png)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndisplay(countsDF.filter(col(\"NAME\").like(\"%TX%\")))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Wide Transformations\n\nThe data required to compute the records in a single partition may reside in many partitions of the parent Dataframe. These operations require that data is **shuffled** between executors.\n\nExamples include:\n* `distinct()`\n* `groupBy(..).sum()`\n* `repartition(n)`\n\n![](https://databricks.com/wp-content/uploads/2018/05/Wide-Transformation.png)"],"metadata":{}},{"cell_type":"code","source":["display(countsDF.groupBy(\"UNIT\").sum(\"counts\"))"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"2.Actions-and-transformations","notebookId":1947196994889101},"nbformat":4,"nbformat_minor":0}
