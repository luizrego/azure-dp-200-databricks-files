{"cells":[{"cell_type":"markdown","source":["# Reading and Writing to Azure Synapse Analytics\n**Technical Accomplishments:**\n- Access an Azure Synapse Analytics warehouse using the SQL Data Warehouse connector\n\n**Requirements:**\n- Databricks Runtime 4.0 or above\n- A database master key for Azure Synapse Analytics"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Azure Synapse Analytics\nAzure Synapse Analytics leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\nImport big data into Azure Synapse Analytics with simple PolyBase T-SQL queries, and then use MPP to run high-performance analytics.\n\nAs you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) SQL Data Warehouse Connector\n\n- Use Azure Blob Storage as an intermediary between Azure Databricks and Azure Synapse Analytics\n- In Azure Databricks: triggers Spark jobs to read and write data to Blob Storage\n- In Azure Synapse Analytics: triggers data loading and unloading operations, performed by **PolyBase**\n\n**Note:** The SQL DW connector is more suited to ETL than to interactive queries.  \nFor interactive and ad-hoc queries, data should be extracted into a Databricks Delta table."],"metadata":{}},{"cell_type":"markdown","source":["![Azure Databricks and Synapse Analytics](https://databricksdemostore.blob.core.windows.net/images/14-de-learning-path/databricks-synapse.png)"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Types of Connections in Azure Synapse Analytics\n\n### **Spark Driver to Azure Synapse Analytics**\nSpark driver connects to Azure Synapse Analytics via JDBC using a username and password.\n\n### **Spark Driver and Executors to Azure Blob Storage**\nSpark uses the **Azure Blob Storage connector** bundled in Databricks Runtime to connect to the Blob Storage container.\n  - Requires **`wasbs`** URI scheme to specify connection\n  - Requires **storage account access key** to set up connection\n    - Set in a notebook's session configuration, which doesn't affect other notebooks attached to the same cluster\n    - **`spark`** is the SparkSession object provided in the notebook\n\n### **Azure Synapse Analytics to Azure Blob Storage**\nSQL DW connector forwards the access key from notebook session configuration to an Azure Synapse Analytics instance over JDBC.\n  - Requires **`forwardSparkAzureStorageCredentials`** set to **`true`**\n  - Represents access key with a temporary <a href=\"https://docs.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-2017\" target=\"_blank\">database scoped credential</a> in the Azure Synapse Analytics instance\n  - Creates a database scoped credential before asking Azure Synapse Analytics to load or unload data, and deletes after loading/unloading is finished"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Enabling access for a notebook session"],"metadata":{}},{"cell_type":"markdown","source":["You can enable access for the lifetime of your notebook session to SQL Data Warehouse by executing the cell below. Be sure to replace the **\"name-of-your-storage-account\"** and **\"your-storage-key\"** values with your own before executing."],"metadata":{}},{"cell_type":"code","source":["storage_account_name = \"name-of-your-storage-account\"\nstorage_account_key = \"your-storage-key\"\nstorage_container_name = \"data\""],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["You will need the JDBC connection string for your Azure Synapse Analytics service. You should copy this value exactly as it appears in the Azure Portal.\n\n**Paste your JDBC connection string** into the empty quotation marks below. Please make sure you have replaced `{your_password_here}` with your SQL Server password."],"metadata":{}},{"cell_type":"code","source":["jdbcURI = \"\""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read from the Customer Table\n\nUse the SQL DW Connector to read data from the Customer Table.\n\nUse the read to define a tempory table that can be queried.\n\nNote the following options in the DataFrameReader in the cell below:\n* **`url`** specifies the JDBC connection to Azure Synapse Analytics\n* **`tempDir`** specifies the **`wasbs`** URI of the caching directory on the Azure Blob Storage container\n* **`forwardSparkAzureStorageCredentials`** is set to **`true`** to ensure that the Azure storage account access keys are forwarded from the notebook's session configuration to the Azure Synapse Analytics"],"metadata":{}},{"cell_type":"code","source":["cacheDir = \"wasbs://{}@{}.blob.core.windows.net/cacheDir\".format(storage_container_name, storage_account_name)\n\nspark_config_key = \"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_account_name)\nspark_config_value = storage_account_key\n\nspark.conf.set(spark_config_key, spark_config_value)\n\ntableName = \"dbo.DimCustomer\"\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Use SQL queries to count the number of rows in the Customer table and to display table metadata."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect count(*) from customer_data"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\ndescribe customer_data"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Note that **`CustomerKey`** and **`CustomerAlternateKey`** use a very similar naming convention."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_data limit 10;"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["When merging many new customers into this table, we may have issues with uniqueness in the **`CustomerKey`**. \n\nLet's redefine **`CustomerAlternateKey`** for stronger uniqueness using a <a href=\"https://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\">UUID</a>. To do this, we will define a UDF and use it to transform the **`CustomerAlternateKey`** column."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport uuid\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Use the Polybase Connector to Write to the Staging Table\n\nUse the SQL DW Connector to write the updated customer table to a staging table.\n\nIt is best practice to update Azure Synapse Analytics via a staging table.\n\nNote the following options in the DataFrameWriter in the cell below:\n* **`url`** specifies the JDBC connection to Azure Synapse Analytics\n* **`tempDir`** specifies the **`wasbs`** URI of the caching directory on the Azure Blob Storage container\n* **`forwardSparkAzureStorageCredentials`** is set to **`true`** to ensure that the Azure storage account access keys are forwarded from the notebook's session configuration to Azure Synapse Analytics\n\nThese options are the same as those in the DataFrameReader above."],"metadata":{}},{"cell_type":"code","source":["(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read From the New Staging Table\nUse the SQL DW Connector to read the new table we just wrote.\n\nUse the read to define a tempory table that can be queried."],"metadata":{}},{"cell_type":"code","source":["customerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;"],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"1.Azure-Synapse-Analytics","notebookId":1947196994887962},"nbformat":4,"nbformat_minor":0}
